{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac7abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf440ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('oneliners_data.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a425acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff2242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "970cfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb46e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbfbdc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1d6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "       \n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a8f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() \n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fff2c11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(96, 512, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=96, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=4\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380b5279",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 10... Loss: 3.3051... Val Loss: 3.2582\n",
      "Epoch: 1/100... Step: 20... Loss: 3.2393... Val Loss: 3.2207\n",
      "Epoch: 1/100... Step: 30... Loss: 3.2222... Val Loss: 3.2116\n",
      "Epoch: 1/100... Step: 40... Loss: 3.2135... Val Loss: 3.2084\n",
      "Epoch: 1/100... Step: 50... Loss: 3.2100... Val Loss: 3.2078\n",
      "Epoch: 1/100... Step: 60... Loss: 3.1984... Val Loss: 3.2079\n",
      "Epoch: 1/100... Step: 70... Loss: 3.2106... Val Loss: 3.2076\n",
      "Epoch: 1/100... Step: 80... Loss: 3.1767... Val Loss: 3.2070\n",
      "Epoch: 2/100... Step: 90... Loss: 3.1945... Val Loss: 3.2065\n",
      "Epoch: 2/100... Step: 100... Loss: 3.1984... Val Loss: 3.2066\n",
      "Epoch: 2/100... Step: 110... Loss: 3.1873... Val Loss: 3.2068\n",
      "Epoch: 2/100... Step: 120... Loss: 3.2181... Val Loss: 3.2068\n",
      "Epoch: 2/100... Step: 130... Loss: 3.1803... Val Loss: 3.2071\n",
      "Epoch: 2/100... Step: 140... Loss: 3.2000... Val Loss: 3.2067\n",
      "Epoch: 2/100... Step: 150... Loss: 3.1893... Val Loss: 3.2064\n",
      "Epoch: 2/100... Step: 160... Loss: 3.1924... Val Loss: 3.2067\n",
      "Epoch: 3/100... Step: 170... Loss: 3.1901... Val Loss: 3.2064\n",
      "Epoch: 3/100... Step: 180... Loss: 3.1666... Val Loss: 3.2069\n",
      "Epoch: 3/100... Step: 190... Loss: 3.1819... Val Loss: 3.2070\n",
      "Epoch: 3/100... Step: 200... Loss: 3.1905... Val Loss: 3.2069\n",
      "Epoch: 3/100... Step: 210... Loss: 3.1848... Val Loss: 3.2067\n",
      "Epoch: 3/100... Step: 220... Loss: 3.2033... Val Loss: 3.2065\n",
      "Epoch: 3/100... Step: 230... Loss: 3.2003... Val Loss: 3.2068\n",
      "Epoch: 3/100... Step: 240... Loss: 3.1999... Val Loss: 3.2069\n",
      "Epoch: 3/100... Step: 250... Loss: 3.1947... Val Loss: 3.2065\n",
      "Epoch: 4/100... Step: 260... Loss: 3.1731... Val Loss: 3.2061\n",
      "Epoch: 4/100... Step: 270... Loss: 3.1896... Val Loss: 3.2062\n",
      "Epoch: 4/100... Step: 280... Loss: 3.1841... Val Loss: 3.2064\n",
      "Epoch: 4/100... Step: 290... Loss: 3.1898... Val Loss: 3.2062\n",
      "Epoch: 4/100... Step: 300... Loss: 3.1878... Val Loss: 3.2067\n",
      "Epoch: 4/100... Step: 310... Loss: 3.1847... Val Loss: 3.2067\n",
      "Epoch: 4/100... Step: 320... Loss: 3.1594... Val Loss: 3.2064\n",
      "Epoch: 4/100... Step: 330... Loss: 3.1970... Val Loss: 3.2066\n",
      "Epoch: 5/100... Step: 340... Loss: 3.2035... Val Loss: 3.2063\n",
      "Epoch: 5/100... Step: 350... Loss: 3.1806... Val Loss: 3.2061\n",
      "Epoch: 5/100... Step: 360... Loss: 3.1884... Val Loss: 3.2067\n",
      "Epoch: 5/100... Step: 370... Loss: 3.1840... Val Loss: 3.2065\n",
      "Epoch: 5/100... Step: 380... Loss: 3.1724... Val Loss: 3.2064\n",
      "Epoch: 5/100... Step: 390... Loss: 3.2029... Val Loss: 3.2055\n",
      "Epoch: 5/100... Step: 400... Loss: 3.2077... Val Loss: 3.2029\n",
      "Epoch: 5/100... Step: 410... Loss: 3.1633... Val Loss: 3.1841\n",
      "Epoch: 5/100... Step: 420... Loss: 3.1220... Val Loss: 3.1130\n",
      "Epoch: 6/100... Step: 430... Loss: 3.0154... Val Loss: 2.9952\n",
      "Epoch: 6/100... Step: 440... Loss: 2.9061... Val Loss: 2.8354\n",
      "Epoch: 6/100... Step: 450... Loss: 2.7545... Val Loss: 2.7525\n",
      "Epoch: 6/100... Step: 460... Loss: 2.6556... Val Loss: 2.6287\n",
      "Epoch: 6/100... Step: 470... Loss: 2.6110... Val Loss: 2.5320\n",
      "Epoch: 6/100... Step: 480... Loss: 2.5313... Val Loss: 2.4655\n",
      "Epoch: 6/100... Step: 490... Loss: 2.4912... Val Loss: 2.4186\n",
      "Epoch: 6/100... Step: 500... Loss: 2.4252... Val Loss: 2.3925\n",
      "Epoch: 7/100... Step: 510... Loss: 2.4056... Val Loss: 2.3545\n",
      "Epoch: 7/100... Step: 520... Loss: 2.3774... Val Loss: 2.3346\n",
      "Epoch: 7/100... Step: 530... Loss: 2.3630... Val Loss: 2.3182\n",
      "Epoch: 7/100... Step: 540... Loss: 2.3689... Val Loss: 2.2916\n",
      "Epoch: 7/100... Step: 550... Loss: 2.2961... Val Loss: 2.2816\n",
      "Epoch: 7/100... Step: 560... Loss: 2.3104... Val Loss: 2.2621\n",
      "Epoch: 7/100... Step: 570... Loss: 2.2997... Val Loss: 2.2431\n",
      "Epoch: 7/100... Step: 580... Loss: 2.2591... Val Loss: 2.2246\n",
      "Epoch: 8/100... Step: 590... Loss: 2.2500... Val Loss: 2.2082\n",
      "Epoch: 8/100... Step: 600... Loss: 2.2358... Val Loss: 2.2220\n",
      "Epoch: 8/100... Step: 610... Loss: 2.2564... Val Loss: 2.2016\n",
      "Epoch: 8/100... Step: 620... Loss: 2.2438... Val Loss: 2.1795\n",
      "Epoch: 8/100... Step: 630... Loss: 2.2196... Val Loss: 2.1645\n",
      "Epoch: 8/100... Step: 640... Loss: 2.2246... Val Loss: 2.1563\n",
      "Epoch: 8/100... Step: 650... Loss: 2.1877... Val Loss: 2.1480\n",
      "Epoch: 8/100... Step: 660... Loss: 2.1913... Val Loss: 2.1389\n",
      "Epoch: 8/100... Step: 670... Loss: 2.1806... Val Loss: 2.1269\n",
      "Epoch: 9/100... Step: 680... Loss: 2.1525... Val Loss: 2.1173\n",
      "Epoch: 9/100... Step: 690... Loss: 2.1587... Val Loss: 2.1099\n",
      "Epoch: 9/100... Step: 700... Loss: 2.1724... Val Loss: 2.1019\n",
      "Epoch: 9/100... Step: 710... Loss: 2.1677... Val Loss: 2.1039\n",
      "Epoch: 9/100... Step: 720... Loss: 2.1157... Val Loss: 2.0927\n",
      "Epoch: 9/100... Step: 730... Loss: 2.1157... Val Loss: 2.0775\n",
      "Epoch: 9/100... Step: 740... Loss: 2.0943... Val Loss: 2.0679\n",
      "Epoch: 9/100... Step: 750... Loss: 2.1419... Val Loss: 2.0711\n",
      "Epoch: 10/100... Step: 760... Loss: 2.1291... Val Loss: 2.0536\n",
      "Epoch: 10/100... Step: 770... Loss: 2.0790... Val Loss: 2.0439\n",
      "Epoch: 10/100... Step: 780... Loss: 2.0837... Val Loss: 2.0419\n",
      "Epoch: 10/100... Step: 790... Loss: 2.0763... Val Loss: 2.0308\n",
      "Epoch: 10/100... Step: 800... Loss: 2.0612... Val Loss: 2.0188\n",
      "Epoch: 10/100... Step: 810... Loss: 2.0861... Val Loss: 2.0161\n",
      "Epoch: 10/100... Step: 820... Loss: 2.0985... Val Loss: 2.0055\n",
      "Epoch: 10/100... Step: 830... Loss: 2.0665... Val Loss: 1.9961\n",
      "Epoch: 10/100... Step: 840... Loss: 2.0603... Val Loss: 1.9925\n",
      "Epoch: 11/100... Step: 850... Loss: 2.0302... Val Loss: 1.9907\n",
      "Epoch: 11/100... Step: 860... Loss: 2.0621... Val Loss: 1.9718\n",
      "Epoch: 11/100... Step: 870... Loss: 2.0185... Val Loss: 1.9665\n",
      "Epoch: 11/100... Step: 880... Loss: 2.0354... Val Loss: 1.9591\n",
      "Epoch: 11/100... Step: 890... Loss: 2.0396... Val Loss: 1.9525\n",
      "Epoch: 11/100... Step: 900... Loss: 2.0363... Val Loss: 1.9469\n",
      "Epoch: 11/100... Step: 910... Loss: 2.0037... Val Loss: 1.9452\n",
      "Epoch: 11/100... Step: 920... Loss: 2.0084... Val Loss: 1.9266\n",
      "Epoch: 12/100... Step: 930... Loss: 2.0039... Val Loss: 1.9192\n",
      "Epoch: 12/100... Step: 940... Loss: 1.9844... Val Loss: 1.9135\n",
      "Epoch: 12/100... Step: 950... Loss: 2.0020... Val Loss: 1.9044\n",
      "Epoch: 12/100... Step: 960... Loss: 2.0092... Val Loss: 1.8995\n",
      "Epoch: 12/100... Step: 970... Loss: 1.9551... Val Loss: 1.8922\n",
      "Epoch: 12/100... Step: 980... Loss: 1.9798... Val Loss: 1.8884\n",
      "Epoch: 12/100... Step: 990... Loss: 1.9718... Val Loss: 1.8757\n",
      "Epoch: 12/100... Step: 1000... Loss: 1.9427... Val Loss: 1.8697\n",
      "Epoch: 13/100... Step: 1010... Loss: 1.9402... Val Loss: 1.8633\n",
      "Epoch: 13/100... Step: 1020... Loss: 1.9193... Val Loss: 1.8562\n",
      "Epoch: 13/100... Step: 1030... Loss: 1.9353... Val Loss: 1.8522\n",
      "Epoch: 13/100... Step: 1040... Loss: 1.9358... Val Loss: 1.8445\n",
      "Epoch: 13/100... Step: 1050... Loss: 1.9275... Val Loss: 1.8365\n",
      "Epoch: 13/100... Step: 1060... Loss: 1.9329... Val Loss: 1.8304\n",
      "Epoch: 13/100... Step: 1070... Loss: 1.9199... Val Loss: 1.8247\n",
      "Epoch: 13/100... Step: 1080... Loss: 1.9073... Val Loss: 1.8193\n",
      "Epoch: 13/100... Step: 1090... Loss: 1.8903... Val Loss: 1.8104\n",
      "Epoch: 14/100... Step: 1100... Loss: 1.8906... Val Loss: 1.8059\n",
      "Epoch: 14/100... Step: 1110... Loss: 1.8772... Val Loss: 1.8034\n",
      "Epoch: 14/100... Step: 1120... Loss: 1.9197... Val Loss: 1.8108\n",
      "Epoch: 14/100... Step: 1130... Loss: 1.8759... Val Loss: 1.7876\n",
      "Epoch: 14/100... Step: 1140... Loss: 1.8469... Val Loss: 1.7830\n",
      "Epoch: 14/100... Step: 1150... Loss: 1.8462... Val Loss: 1.7747\n",
      "Epoch: 14/100... Step: 1160... Loss: 1.8260... Val Loss: 1.7704\n",
      "Epoch: 14/100... Step: 1170... Loss: 1.8535... Val Loss: 1.7631\n",
      "Epoch: 15/100... Step: 1180... Loss: 1.8730... Val Loss: 1.7561\n",
      "Epoch: 15/100... Step: 1190... Loss: 1.8316... Val Loss: 1.7510\n",
      "Epoch: 15/100... Step: 1200... Loss: 1.8192... Val Loss: 1.7447\n",
      "Epoch: 15/100... Step: 1210... Loss: 1.8074... Val Loss: 1.7389\n",
      "Epoch: 15/100... Step: 1220... Loss: 1.8041... Val Loss: 1.7340\n",
      "Epoch: 15/100... Step: 1230... Loss: 1.8231... Val Loss: 1.7276\n",
      "Epoch: 15/100... Step: 1240... Loss: 1.8284... Val Loss: 1.7209\n",
      "Epoch: 15/100... Step: 1250... Loss: 1.8136... Val Loss: 1.7189\n",
      "Epoch: 15/100... Step: 1260... Loss: 1.8145... Val Loss: 1.7139\n",
      "Epoch: 16/100... Step: 1270... Loss: 1.7955... Val Loss: 1.7073\n",
      "Epoch: 16/100... Step: 1280... Loss: 1.8035... Val Loss: 1.7002\n",
      "Epoch: 16/100... Step: 1290... Loss: 1.7755... Val Loss: 1.6976\n",
      "Epoch: 16/100... Step: 1300... Loss: 1.7952... Val Loss: 1.6940\n",
      "Epoch: 16/100... Step: 1310... Loss: 1.7965... Val Loss: 1.6890\n",
      "Epoch: 16/100... Step: 1320... Loss: 1.8033... Val Loss: 1.6848\n",
      "Epoch: 16/100... Step: 1330... Loss: 1.7644... Val Loss: 1.6780\n",
      "Epoch: 16/100... Step: 1340... Loss: 1.7810... Val Loss: 1.6755\n",
      "Epoch: 17/100... Step: 1350... Loss: 1.7704... Val Loss: 1.6690\n",
      "Epoch: 17/100... Step: 1360... Loss: 1.7563... Val Loss: 1.6674\n",
      "Epoch: 17/100... Step: 1370... Loss: 1.7743... Val Loss: 1.6631\n",
      "Epoch: 17/100... Step: 1380... Loss: 1.7660... Val Loss: 1.6604\n",
      "Epoch: 17/100... Step: 1390... Loss: 1.7220... Val Loss: 1.6548\n",
      "Epoch: 17/100... Step: 1400... Loss: 1.7463... Val Loss: 1.6503\n",
      "Epoch: 17/100... Step: 1410... Loss: 1.7365... Val Loss: 1.6440\n",
      "Epoch: 17/100... Step: 1420... Loss: 1.7318... Val Loss: 1.6412\n",
      "Epoch: 18/100... Step: 1430... Loss: 1.7208... Val Loss: 1.6385\n",
      "Epoch: 18/100... Step: 1440... Loss: 1.7094... Val Loss: 1.6352\n",
      "Epoch: 18/100... Step: 1450... Loss: 1.7179... Val Loss: 1.6345\n",
      "Epoch: 18/100... Step: 1460... Loss: 1.7237... Val Loss: 1.6277\n",
      "Epoch: 18/100... Step: 1470... Loss: 1.7177... Val Loss: 1.6257\n",
      "Epoch: 18/100... Step: 1480... Loss: 1.7169... Val Loss: 1.6177\n",
      "Epoch: 18/100... Step: 1490... Loss: 1.7140... Val Loss: 1.6166\n",
      "Epoch: 18/100... Step: 1500... Loss: 1.7224... Val Loss: 1.6103\n",
      "Epoch: 18/100... Step: 1510... Loss: 1.6883... Val Loss: 1.6074\n",
      "Epoch: 19/100... Step: 1520... Loss: 1.6954... Val Loss: 1.6051\n",
      "Epoch: 19/100... Step: 1530... Loss: 1.6822... Val Loss: 1.6015\n",
      "Epoch: 19/100... Step: 1540... Loss: 1.7210... Val Loss: 1.5983\n",
      "Epoch: 19/100... Step: 1550... Loss: 1.6859... Val Loss: 1.5935\n",
      "Epoch: 19/100... Step: 1560... Loss: 1.6641... Val Loss: 1.5939\n",
      "Epoch: 19/100... Step: 1570... Loss: 1.6723... Val Loss: 1.5890\n",
      "Epoch: 19/100... Step: 1580... Loss: 1.6509... Val Loss: 1.5875\n",
      "Epoch: 19/100... Step: 1590... Loss: 1.6865... Val Loss: 1.5824\n",
      "Epoch: 20/100... Step: 1600... Loss: 1.6995... Val Loss: 1.5786\n",
      "Epoch: 20/100... Step: 1610... Loss: 1.6524... Val Loss: 1.5768\n",
      "Epoch: 20/100... Step: 1620... Loss: 1.6509... Val Loss: 1.5726\n",
      "Epoch: 20/100... Step: 1630... Loss: 1.6424... Val Loss: 1.5711\n",
      "Epoch: 20/100... Step: 1640... Loss: 1.6436... Val Loss: 1.5687\n",
      "Epoch: 20/100... Step: 1650... Loss: 1.6548... Val Loss: 1.5631\n",
      "Epoch: 20/100... Step: 1660... Loss: 1.6602... Val Loss: 1.5602\n",
      "Epoch: 20/100... Step: 1670... Loss: 1.6516... Val Loss: 1.5560\n",
      "Epoch: 20/100... Step: 1680... Loss: 1.6634... Val Loss: 1.5570\n",
      "Epoch: 21/100... Step: 1690... Loss: 1.6456... Val Loss: 1.5542\n",
      "Epoch: 21/100... Step: 1700... Loss: 1.6423... Val Loss: 1.5514\n",
      "Epoch: 21/100... Step: 1710... Loss: 1.6232... Val Loss: 1.5476\n",
      "Epoch: 21/100... Step: 1720... Loss: 1.6507... Val Loss: 1.5444\n",
      "Epoch: 21/100... Step: 1730... Loss: 1.6411... Val Loss: 1.5419\n",
      "Epoch: 21/100... Step: 1740... Loss: 1.6647... Val Loss: 1.5386\n",
      "Epoch: 21/100... Step: 1750... Loss: 1.6164... Val Loss: 1.5382\n",
      "Epoch: 21/100... Step: 1760... Loss: 1.6382... Val Loss: 1.5343\n",
      "Epoch: 22/100... Step: 1770... Loss: 1.6365... Val Loss: 1.5319\n",
      "Epoch: 22/100... Step: 1780... Loss: 1.6178... Val Loss: 1.5280\n",
      "Epoch: 22/100... Step: 1790... Loss: 1.6337... Val Loss: 1.5254\n",
      "Epoch: 22/100... Step: 1800... Loss: 1.6367... Val Loss: 1.5240\n",
      "Epoch: 22/100... Step: 1810... Loss: 1.5897... Val Loss: 1.5211\n",
      "Epoch: 22/100... Step: 1820... Loss: 1.6104... Val Loss: 1.5209\n",
      "Epoch: 22/100... Step: 1830... Loss: 1.5994... Val Loss: 1.5165\n",
      "Epoch: 22/100... Step: 1840... Loss: 1.5890... Val Loss: 1.5156\n",
      "Epoch: 23/100... Step: 1850... Loss: 1.5850... Val Loss: 1.5143\n",
      "Epoch: 23/100... Step: 1860... Loss: 1.5836... Val Loss: 1.5104\n",
      "Epoch: 23/100... Step: 1870... Loss: 1.5859... Val Loss: 1.5084\n",
      "Epoch: 23/100... Step: 1880... Loss: 1.6012... Val Loss: 1.5052\n",
      "Epoch: 23/100... Step: 1890... Loss: 1.6019... Val Loss: 1.5056\n",
      "Epoch: 23/100... Step: 1900... Loss: 1.6024... Val Loss: 1.5005\n",
      "Epoch: 23/100... Step: 1910... Loss: 1.5973... Val Loss: 1.4991\n",
      "Epoch: 23/100... Step: 1920... Loss: 1.5925... Val Loss: 1.4957\n",
      "Epoch: 23/100... Step: 1930... Loss: 1.5700... Val Loss: 1.4947\n",
      "Epoch: 24/100... Step: 1940... Loss: 1.5781... Val Loss: 1.4930\n",
      "Epoch: 24/100... Step: 1950... Loss: 1.5756... Val Loss: 1.4906\n",
      "Epoch: 24/100... Step: 1960... Loss: 1.5995... Val Loss: 1.4911\n",
      "Epoch: 24/100... Step: 1970... Loss: 1.5715... Val Loss: 1.4851\n",
      "Epoch: 24/100... Step: 1980... Loss: 1.5540... Val Loss: 1.4839\n",
      "Epoch: 24/100... Step: 1990... Loss: 1.5674... Val Loss: 1.4829\n",
      "Epoch: 24/100... Step: 2000... Loss: 1.5336... Val Loss: 1.4811\n",
      "Epoch: 24/100... Step: 2010... Loss: 1.5814... Val Loss: 1.4796\n",
      "Epoch: 25/100... Step: 2020... Loss: 1.5912... Val Loss: 1.4766\n",
      "Epoch: 25/100... Step: 2030... Loss: 1.5529... Val Loss: 1.4746\n",
      "Epoch: 25/100... Step: 2040... Loss: 1.5410... Val Loss: 1.4733\n",
      "Epoch: 25/100... Step: 2050... Loss: 1.5417... Val Loss: 1.4724\n",
      "Epoch: 25/100... Step: 2060... Loss: 1.5518... Val Loss: 1.4695\n",
      "Epoch: 25/100... Step: 2070... Loss: 1.5490... Val Loss: 1.4679\n",
      "Epoch: 25/100... Step: 2080... Loss: 1.5593... Val Loss: 1.4656\n",
      "Epoch: 25/100... Step: 2090... Loss: 1.5470... Val Loss: 1.4640\n",
      "Epoch: 25/100... Step: 2100... Loss: 1.5681... Val Loss: 1.4618\n",
      "Epoch: 26/100... Step: 2110... Loss: 1.5466... Val Loss: 1.4625\n",
      "Epoch: 26/100... Step: 2120... Loss: 1.5346... Val Loss: 1.4583\n",
      "Epoch: 26/100... Step: 2130... Loss: 1.5221... Val Loss: 1.4567\n",
      "Epoch: 26/100... Step: 2140... Loss: 1.5539... Val Loss: 1.4566\n",
      "Epoch: 26/100... Step: 2150... Loss: 1.5436... Val Loss: 1.4525\n",
      "Epoch: 26/100... Step: 2160... Loss: 1.5675... Val Loss: 1.4542\n",
      "Epoch: 26/100... Step: 2170... Loss: 1.5250... Val Loss: 1.4486\n",
      "Epoch: 26/100... Step: 2180... Loss: 1.5487... Val Loss: 1.4485\n",
      "Epoch: 27/100... Step: 2190... Loss: 1.5499... Val Loss: 1.4455\n",
      "Epoch: 27/100... Step: 2200... Loss: 1.5256... Val Loss: 1.4471\n",
      "Epoch: 27/100... Step: 2210... Loss: 1.5473... Val Loss: 1.4407\n",
      "Epoch: 27/100... Step: 2220... Loss: 1.5559... Val Loss: 1.4429\n",
      "Epoch: 27/100... Step: 2230... Loss: 1.5036... Val Loss: 1.4394\n",
      "Epoch: 27/100... Step: 2240... Loss: 1.5278... Val Loss: 1.4384\n",
      "Epoch: 27/100... Step: 2250... Loss: 1.5139... Val Loss: 1.4369\n",
      "Epoch: 27/100... Step: 2260... Loss: 1.5065... Val Loss: 1.4367\n",
      "Epoch: 28/100... Step: 2270... Loss: 1.5025... Val Loss: 1.4340\n",
      "Epoch: 28/100... Step: 2280... Loss: 1.5106... Val Loss: 1.4338\n",
      "Epoch: 28/100... Step: 2290... Loss: 1.5073... Val Loss: 1.4316\n",
      "Epoch: 28/100... Step: 2300... Loss: 1.5292... Val Loss: 1.4295\n",
      "Epoch: 28/100... Step: 2310... Loss: 1.5175... Val Loss: 1.4296\n",
      "Epoch: 28/100... Step: 2320... Loss: 1.5202... Val Loss: 1.4255\n",
      "Epoch: 28/100... Step: 2330... Loss: 1.5132... Val Loss: 1.4256\n",
      "Epoch: 28/100... Step: 2340... Loss: 1.5152... Val Loss: 1.4217\n",
      "Epoch: 28/100... Step: 2350... Loss: 1.4971... Val Loss: 1.4232\n",
      "Epoch: 29/100... Step: 2360... Loss: 1.5060... Val Loss: 1.4200\n",
      "Epoch: 29/100... Step: 2370... Loss: 1.5009... Val Loss: 1.4212\n",
      "Epoch: 29/100... Step: 2380... Loss: 1.5215... Val Loss: 1.4159\n",
      "Epoch: 29/100... Step: 2390... Loss: 1.4877... Val Loss: 1.4166\n",
      "Epoch: 29/100... Step: 2400... Loss: 1.4742... Val Loss: 1.4144\n",
      "Epoch: 29/100... Step: 2410... Loss: 1.4861... Val Loss: 1.4137\n",
      "Epoch: 29/100... Step: 2420... Loss: 1.4621... Val Loss: 1.4129\n",
      "Epoch: 29/100... Step: 2430... Loss: 1.5091... Val Loss: 1.4113\n",
      "Epoch: 30/100... Step: 2440... Loss: 1.5099... Val Loss: 1.4093\n",
      "Epoch: 30/100... Step: 2450... Loss: 1.4756... Val Loss: 1.4086\n",
      "Epoch: 30/100... Step: 2460... Loss: 1.4687... Val Loss: 1.4089\n",
      "Epoch: 30/100... Step: 2470... Loss: 1.4650... Val Loss: 1.4057\n",
      "Epoch: 30/100... Step: 2480... Loss: 1.4811... Val Loss: 1.4050\n",
      "Epoch: 30/100... Step: 2490... Loss: 1.4794... Val Loss: 1.4037\n",
      "Epoch: 30/100... Step: 2500... Loss: 1.4894... Val Loss: 1.4020\n",
      "Epoch: 30/100... Step: 2510... Loss: 1.4750... Val Loss: 1.4014\n",
      "Epoch: 30/100... Step: 2520... Loss: 1.5011... Val Loss: 1.4000\n",
      "Epoch: 31/100... Step: 2530... Loss: 1.4776... Val Loss: 1.4013\n",
      "Epoch: 31/100... Step: 2540... Loss: 1.4613... Val Loss: 1.3972\n",
      "Epoch: 31/100... Step: 2550... Loss: 1.4548... Val Loss: 1.3963\n",
      "Epoch: 31/100... Step: 2560... Loss: 1.4830... Val Loss: 1.3953\n",
      "Epoch: 31/100... Step: 2570... Loss: 1.4732... Val Loss: 1.3959\n",
      "Epoch: 31/100... Step: 2580... Loss: 1.5080... Val Loss: 1.3945\n",
      "Epoch: 31/100... Step: 2590... Loss: 1.4609... Val Loss: 1.3918\n",
      "Epoch: 31/100... Step: 2600... Loss: 1.4807... Val Loss: 1.3911\n",
      "Epoch: 32/100... Step: 2610... Loss: 1.4836... Val Loss: 1.3890\n",
      "Epoch: 32/100... Step: 2620... Loss: 1.4711... Val Loss: 1.3888\n",
      "Epoch: 32/100... Step: 2630... Loss: 1.4783... Val Loss: 1.3858\n",
      "Epoch: 32/100... Step: 2640... Loss: 1.4857... Val Loss: 1.3855\n",
      "Epoch: 32/100... Step: 2650... Loss: 1.4443... Val Loss: 1.3839\n",
      "Epoch: 32/100... Step: 2660... Loss: 1.4639... Val Loss: 1.3850\n",
      "Epoch: 32/100... Step: 2670... Loss: 1.4499... Val Loss: 1.3821\n",
      "Epoch: 32/100... Step: 2680... Loss: 1.4459... Val Loss: 1.3853\n",
      "Epoch: 33/100... Step: 2690... Loss: 1.4431... Val Loss: 1.3813\n",
      "Epoch: 33/100... Step: 2700... Loss: 1.4477... Val Loss: 1.3824\n",
      "Epoch: 33/100... Step: 2710... Loss: 1.4404... Val Loss: 1.3779\n",
      "Epoch: 33/100... Step: 2720... Loss: 1.4674... Val Loss: 1.3771\n",
      "Epoch: 33/100... Step: 2730... Loss: 1.4668... Val Loss: 1.3768\n",
      "Epoch: 33/100... Step: 2740... Loss: 1.4554... Val Loss: 1.3752\n",
      "Epoch: 33/100... Step: 2750... Loss: 1.4528... Val Loss: 1.3729\n",
      "Epoch: 33/100... Step: 2760... Loss: 1.4596... Val Loss: 1.3738\n",
      "Epoch: 33/100... Step: 2770... Loss: 1.4380... Val Loss: 1.3737\n",
      "Epoch: 34/100... Step: 2780... Loss: 1.4492... Val Loss: 1.3708\n",
      "Epoch: 34/100... Step: 2790... Loss: 1.4381... Val Loss: 1.3721\n",
      "Epoch: 34/100... Step: 2800... Loss: 1.4655... Val Loss: 1.3694\n",
      "Epoch: 34/100... Step: 2810... Loss: 1.4302... Val Loss: 1.3683\n",
      "Epoch: 34/100... Step: 2820... Loss: 1.4122... Val Loss: 1.3684\n",
      "Epoch: 34/100... Step: 2830... Loss: 1.4328... Val Loss: 1.3676\n",
      "Epoch: 34/100... Step: 2840... Loss: 1.4085... Val Loss: 1.3669\n",
      "Epoch: 34/100... Step: 2850... Loss: 1.4546... Val Loss: 1.3666\n",
      "Epoch: 35/100... Step: 2860... Loss: 1.4644... Val Loss: 1.3647\n",
      "Epoch: 35/100... Step: 2870... Loss: 1.4220... Val Loss: 1.3631\n",
      "Epoch: 35/100... Step: 2880... Loss: 1.4172... Val Loss: 1.3643\n",
      "Epoch: 35/100... Step: 2890... Loss: 1.4140... Val Loss: 1.3638\n",
      "Epoch: 35/100... Step: 2900... Loss: 1.4316... Val Loss: 1.3617\n",
      "Epoch: 35/100... Step: 2910... Loss: 1.4286... Val Loss: 1.3597\n",
      "Epoch: 35/100... Step: 2920... Loss: 1.4366... Val Loss: 1.3583\n",
      "Epoch: 35/100... Step: 2930... Loss: 1.4213... Val Loss: 1.3576\n",
      "Epoch: 35/100... Step: 2940... Loss: 1.4447... Val Loss: 1.3573\n",
      "Epoch: 36/100... Step: 2950... Loss: 1.4292... Val Loss: 1.3605\n",
      "Epoch: 36/100... Step: 2960... Loss: 1.4064... Val Loss: 1.3554\n",
      "Epoch: 36/100... Step: 2970... Loss: 1.4056... Val Loss: 1.3551\n",
      "Epoch: 36/100... Step: 2980... Loss: 1.4439... Val Loss: 1.3539\n",
      "Epoch: 36/100... Step: 2990... Loss: 1.4266... Val Loss: 1.3561\n",
      "Epoch: 36/100... Step: 3000... Loss: 1.4567... Val Loss: 1.3529\n",
      "Epoch: 36/100... Step: 3010... Loss: 1.4056... Val Loss: 1.3527\n",
      "Epoch: 36/100... Step: 3020... Loss: 1.4360... Val Loss: 1.3508\n",
      "Epoch: 37/100... Step: 3030... Loss: 1.4379... Val Loss: 1.3503\n",
      "Epoch: 37/100... Step: 3040... Loss: 1.4125... Val Loss: 1.3493\n",
      "Epoch: 37/100... Step: 3050... Loss: 1.4324... Val Loss: 1.3475\n",
      "Epoch: 37/100... Step: 3060... Loss: 1.4348... Val Loss: 1.3474\n",
      "Epoch: 37/100... Step: 3070... Loss: 1.3952... Val Loss: 1.3454\n",
      "Epoch: 37/100... Step: 3080... Loss: 1.4117... Val Loss: 1.3476\n",
      "Epoch: 37/100... Step: 3090... Loss: 1.4103... Val Loss: 1.3452\n",
      "Epoch: 37/100... Step: 3100... Loss: 1.4012... Val Loss: 1.3468\n",
      "Epoch: 38/100... Step: 3110... Loss: 1.3958... Val Loss: 1.3452\n",
      "Epoch: 38/100... Step: 3120... Loss: 1.4048... Val Loss: 1.3463\n",
      "Epoch: 38/100... Step: 3130... Loss: 1.3992... Val Loss: 1.3415\n",
      "Epoch: 38/100... Step: 3140... Loss: 1.4237... Val Loss: 1.3412\n",
      "Epoch: 38/100... Step: 3150... Loss: 1.4166... Val Loss: 1.3421\n",
      "Epoch: 38/100... Step: 3160... Loss: 1.4177... Val Loss: 1.3406\n",
      "Epoch: 38/100... Step: 3170... Loss: 1.4144... Val Loss: 1.3393\n",
      "Epoch: 38/100... Step: 3180... Loss: 1.4050... Val Loss: 1.3366\n",
      "Epoch: 38/100... Step: 3190... Loss: 1.3894... Val Loss: 1.3380\n",
      "Epoch: 39/100... Step: 3200... Loss: 1.4074... Val Loss: 1.3362\n",
      "Epoch: 39/100... Step: 3210... Loss: 1.4107... Val Loss: 1.3358\n",
      "Epoch: 39/100... Step: 3220... Loss: 1.4165... Val Loss: 1.3346\n",
      "Epoch: 39/100... Step: 3230... Loss: 1.3910... Val Loss: 1.3338\n",
      "Epoch: 39/100... Step: 3240... Loss: 1.3755... Val Loss: 1.3343\n",
      "Epoch: 39/100... Step: 3250... Loss: 1.3882... Val Loss: 1.3341\n",
      "Epoch: 39/100... Step: 3260... Loss: 1.3690... Val Loss: 1.3331\n",
      "Epoch: 39/100... Step: 3270... Loss: 1.4127... Val Loss: 1.3331\n",
      "Epoch: 40/100... Step: 3280... Loss: 1.4217... Val Loss: 1.3324\n",
      "Epoch: 40/100... Step: 3290... Loss: 1.3798... Val Loss: 1.3310\n",
      "Epoch: 40/100... Step: 3300... Loss: 1.3744... Val Loss: 1.3314\n",
      "Epoch: 40/100... Step: 3310... Loss: 1.3795... Val Loss: 1.3314\n",
      "Epoch: 40/100... Step: 3320... Loss: 1.3897... Val Loss: 1.3313\n",
      "Epoch: 40/100... Step: 3330... Loss: 1.3884... Val Loss: 1.3279\n",
      "Epoch: 40/100... Step: 3340... Loss: 1.4002... Val Loss: 1.3275\n",
      "Epoch: 40/100... Step: 3350... Loss: 1.3838... Val Loss: 1.3254\n",
      "Epoch: 40/100... Step: 3360... Loss: 1.4054... Val Loss: 1.3271\n",
      "Epoch: 41/100... Step: 3370... Loss: 1.3924... Val Loss: 1.3274\n",
      "Epoch: 41/100... Step: 3380... Loss: 1.3764... Val Loss: 1.3251\n",
      "Epoch: 41/100... Step: 3390... Loss: 1.3671... Val Loss: 1.3238\n",
      "Epoch: 41/100... Step: 3400... Loss: 1.3968... Val Loss: 1.3248\n",
      "Epoch: 41/100... Step: 3410... Loss: 1.3907... Val Loss: 1.3244\n",
      "Epoch: 41/100... Step: 3420... Loss: 1.4167... Val Loss: 1.3231\n",
      "Epoch: 41/100... Step: 3430... Loss: 1.3609... Val Loss: 1.3234\n",
      "Epoch: 41/100... Step: 3440... Loss: 1.3938... Val Loss: 1.3208\n",
      "Epoch: 42/100... Step: 3450... Loss: 1.3929... Val Loss: 1.3213\n",
      "Epoch: 42/100... Step: 3460... Loss: 1.3794... Val Loss: 1.3201\n",
      "Epoch: 42/100... Step: 3470... Loss: 1.3950... Val Loss: 1.3205\n",
      "Epoch: 42/100... Step: 3480... Loss: 1.4048... Val Loss: 1.3193\n",
      "Epoch: 42/100... Step: 3490... Loss: 1.3569... Val Loss: 1.3182\n",
      "Epoch: 42/100... Step: 3500... Loss: 1.3806... Val Loss: 1.3186\n",
      "Epoch: 42/100... Step: 3510... Loss: 1.3678... Val Loss: 1.3190\n",
      "Epoch: 42/100... Step: 3520... Loss: 1.3622... Val Loss: 1.3171\n",
      "Epoch: 43/100... Step: 3530... Loss: 1.3528... Val Loss: 1.3184\n",
      "Epoch: 43/100... Step: 3540... Loss: 1.3601... Val Loss: 1.3190\n",
      "Epoch: 43/100... Step: 3550... Loss: 1.3577... Val Loss: 1.3152\n",
      "Epoch: 43/100... Step: 3560... Loss: 1.3854... Val Loss: 1.3149\n",
      "Epoch: 43/100... Step: 3570... Loss: 1.3877... Val Loss: 1.3154\n",
      "Epoch: 43/100... Step: 3580... Loss: 1.3830... Val Loss: 1.3148\n",
      "Epoch: 43/100... Step: 3590... Loss: 1.3720... Val Loss: 1.3143\n",
      "Epoch: 43/100... Step: 3600... Loss: 1.3756... Val Loss: 1.3115\n",
      "Epoch: 43/100... Step: 3610... Loss: 1.3634... Val Loss: 1.3129\n",
      "Epoch: 44/100... Step: 3620... Loss: 1.3718... Val Loss: 1.3110\n",
      "Epoch: 44/100... Step: 3630... Loss: 1.3613... Val Loss: 1.3117\n",
      "Epoch: 44/100... Step: 3640... Loss: 1.3859... Val Loss: 1.3111\n",
      "Epoch: 44/100... Step: 3650... Loss: 1.3484... Val Loss: 1.3084\n",
      "Epoch: 44/100... Step: 3660... Loss: 1.3389... Val Loss: 1.3097\n",
      "Epoch: 44/100... Step: 3670... Loss: 1.3505... Val Loss: 1.3098\n",
      "Epoch: 44/100... Step: 3680... Loss: 1.3365... Val Loss: 1.3078\n",
      "Epoch: 44/100... Step: 3690... Loss: 1.3785... Val Loss: 1.3114\n",
      "Epoch: 45/100... Step: 3700... Loss: 1.3867... Val Loss: 1.3075\n",
      "Epoch: 45/100... Step: 3710... Loss: 1.3472... Val Loss: 1.3094\n",
      "Epoch: 45/100... Step: 3720... Loss: 1.3459... Val Loss: 1.3085\n",
      "Epoch: 45/100... Step: 3730... Loss: 1.3419... Val Loss: 1.3081\n",
      "Epoch: 45/100... Step: 3740... Loss: 1.3577... Val Loss: 1.3070\n",
      "Epoch: 45/100... Step: 3750... Loss: 1.3592... Val Loss: 1.3054\n",
      "Epoch: 45/100... Step: 3760... Loss: 1.3576... Val Loss: 1.3048\n",
      "Epoch: 45/100... Step: 3770... Loss: 1.3547... Val Loss: 1.3031\n",
      "Epoch: 45/100... Step: 3780... Loss: 1.3770... Val Loss: 1.3050\n",
      "Epoch: 46/100... Step: 3790... Loss: 1.3595... Val Loss: 1.3035\n",
      "Epoch: 46/100... Step: 3800... Loss: 1.3496... Val Loss: 1.3026\n",
      "Epoch: 46/100... Step: 3810... Loss: 1.3400... Val Loss: 1.3016\n",
      "Epoch: 46/100... Step: 3820... Loss: 1.3575... Val Loss: 1.3025\n",
      "Epoch: 46/100... Step: 3830... Loss: 1.3644... Val Loss: 1.3030\n",
      "Epoch: 46/100... Step: 3840... Loss: 1.3885... Val Loss: 1.3013\n",
      "Epoch: 46/100... Step: 3850... Loss: 1.3403... Val Loss: 1.3001\n",
      "Epoch: 46/100... Step: 3860... Loss: 1.3621... Val Loss: 1.3002\n",
      "Epoch: 47/100... Step: 3870... Loss: 1.3658... Val Loss: 1.2983\n",
      "Epoch: 47/100... Step: 3880... Loss: 1.3456... Val Loss: 1.2999\n",
      "Epoch: 47/100... Step: 3890... Loss: 1.3642... Val Loss: 1.2989\n",
      "Epoch: 47/100... Step: 3900... Loss: 1.3696... Val Loss: 1.2982\n",
      "Epoch: 47/100... Step: 3910... Loss: 1.3292... Val Loss: 1.2969\n",
      "Epoch: 47/100... Step: 3920... Loss: 1.3464... Val Loss: 1.2971\n",
      "Epoch: 47/100... Step: 3930... Loss: 1.3407... Val Loss: 1.2983\n",
      "Epoch: 47/100... Step: 3940... Loss: 1.3342... Val Loss: 1.2970\n",
      "Epoch: 48/100... Step: 3950... Loss: 1.3312... Val Loss: 1.2982\n",
      "Epoch: 48/100... Step: 3960... Loss: 1.3296... Val Loss: 1.2980\n",
      "Epoch: 48/100... Step: 3970... Loss: 1.3299... Val Loss: 1.2956\n",
      "Epoch: 48/100... Step: 3980... Loss: 1.3578... Val Loss: 1.2953\n",
      "Epoch: 48/100... Step: 3990... Loss: 1.3512... Val Loss: 1.2972\n",
      "Epoch: 48/100... Step: 4000... Loss: 1.3503... Val Loss: 1.2948\n",
      "Epoch: 48/100... Step: 4010... Loss: 1.3475... Val Loss: 1.2933\n",
      "Epoch: 48/100... Step: 4020... Loss: 1.3480... Val Loss: 1.2927\n",
      "Epoch: 48/100... Step: 4030... Loss: 1.3288... Val Loss: 1.2925\n",
      "Epoch: 49/100... Step: 4040... Loss: 1.3442... Val Loss: 1.2926\n",
      "Epoch: 49/100... Step: 4050... Loss: 1.3389... Val Loss: 1.2922\n",
      "Epoch: 49/100... Step: 4060... Loss: 1.3553... Val Loss: 1.2932\n",
      "Epoch: 49/100... Step: 4070... Loss: 1.3222... Val Loss: 1.2896\n",
      "Epoch: 49/100... Step: 4080... Loss: 1.3162... Val Loss: 1.2909\n",
      "Epoch: 49/100... Step: 4090... Loss: 1.3211... Val Loss: 1.2925\n",
      "Epoch: 49/100... Step: 4100... Loss: 1.3095... Val Loss: 1.2898\n",
      "Epoch: 49/100... Step: 4110... Loss: 1.3636... Val Loss: 1.2910\n",
      "Epoch: 50/100... Step: 4120... Loss: 1.3597... Val Loss: 1.2895\n",
      "Epoch: 50/100... Step: 4130... Loss: 1.3160... Val Loss: 1.2900\n",
      "Epoch: 50/100... Step: 4140... Loss: 1.3112... Val Loss: 1.2900\n",
      "Epoch: 50/100... Step: 4150... Loss: 1.3162... Val Loss: 1.2894\n",
      "Epoch: 50/100... Step: 4160... Loss: 1.3418... Val Loss: 1.2902\n",
      "Epoch: 50/100... Step: 4170... Loss: 1.3273... Val Loss: 1.2881\n",
      "Epoch: 50/100... Step: 4180... Loss: 1.3390... Val Loss: 1.2872\n",
      "Epoch: 50/100... Step: 4190... Loss: 1.3226... Val Loss: 1.2867\n",
      "Epoch: 50/100... Step: 4200... Loss: 1.3541... Val Loss: 1.2879\n",
      "Epoch: 51/100... Step: 4210... Loss: 1.3303... Val Loss: 1.2871\n",
      "Epoch: 51/100... Step: 4220... Loss: 1.3265... Val Loss: 1.2849\n",
      "Epoch: 51/100... Step: 4230... Loss: 1.3105... Val Loss: 1.2849\n",
      "Epoch: 51/100... Step: 4240... Loss: 1.3374... Val Loss: 1.2842\n",
      "Epoch: 51/100... Step: 4250... Loss: 1.3296... Val Loss: 1.2861\n",
      "Epoch: 51/100... Step: 4260... Loss: 1.3593... Val Loss: 1.2849\n",
      "Epoch: 51/100... Step: 4270... Loss: 1.3060... Val Loss: 1.2826\n",
      "Epoch: 51/100... Step: 4280... Loss: 1.3338... Val Loss: 1.2833\n",
      "Epoch: 52/100... Step: 4290... Loss: 1.3413... Val Loss: 1.2830\n",
      "Epoch: 52/100... Step: 4300... Loss: 1.3260... Val Loss: 1.2831\n",
      "Epoch: 52/100... Step: 4310... Loss: 1.3360... Val Loss: 1.2835\n",
      "Epoch: 52/100... Step: 4320... Loss: 1.3429... Val Loss: 1.2829\n",
      "Epoch: 52/100... Step: 4330... Loss: 1.3075... Val Loss: 1.2810\n",
      "Epoch: 52/100... Step: 4340... Loss: 1.3212... Val Loss: 1.2811\n",
      "Epoch: 52/100... Step: 4350... Loss: 1.3076... Val Loss: 1.2836\n",
      "Epoch: 52/100... Step: 4360... Loss: 1.3129... Val Loss: 1.2808\n",
      "Epoch: 53/100... Step: 4370... Loss: 1.3104... Val Loss: 1.2809\n",
      "Epoch: 53/100... Step: 4380... Loss: 1.3084... Val Loss: 1.2820\n",
      "Epoch: 53/100... Step: 4390... Loss: 1.3028... Val Loss: 1.2795\n",
      "Epoch: 53/100... Step: 4400... Loss: 1.3340... Val Loss: 1.2789\n",
      "Epoch: 53/100... Step: 4410... Loss: 1.3337... Val Loss: 1.2803\n",
      "Epoch: 53/100... Step: 4420... Loss: 1.3230... Val Loss: 1.2800\n",
      "Epoch: 53/100... Step: 4430... Loss: 1.3275... Val Loss: 1.2775\n",
      "Epoch: 53/100... Step: 4440... Loss: 1.3236... Val Loss: 1.2771\n",
      "Epoch: 53/100... Step: 4450... Loss: 1.3094... Val Loss: 1.2796\n",
      "Epoch: 54/100... Step: 4460... Loss: 1.3267... Val Loss: 1.2783\n",
      "Epoch: 54/100... Step: 4470... Loss: 1.3163... Val Loss: 1.2771\n",
      "Epoch: 54/100... Step: 4480... Loss: 1.3288... Val Loss: 1.2779\n",
      "Epoch: 54/100... Step: 4490... Loss: 1.3029... Val Loss: 1.2755\n",
      "Epoch: 54/100... Step: 4500... Loss: 1.2971... Val Loss: 1.2755\n",
      "Epoch: 54/100... Step: 4510... Loss: 1.2976... Val Loss: 1.2779\n",
      "Epoch: 54/100... Step: 4520... Loss: 1.2899... Val Loss: 1.2784\n",
      "Epoch: 54/100... Step: 4530... Loss: 1.3353... Val Loss: 1.2764\n",
      "Epoch: 55/100... Step: 4540... Loss: 1.3365... Val Loss: 1.2757\n",
      "Epoch: 55/100... Step: 4550... Loss: 1.2948... Val Loss: 1.2766\n",
      "Epoch: 55/100... Step: 4560... Loss: 1.2947... Val Loss: 1.2779\n",
      "Epoch: 55/100... Step: 4570... Loss: 1.2889... Val Loss: 1.2765\n",
      "Epoch: 55/100... Step: 4580... Loss: 1.3133... Val Loss: 1.2781\n",
      "Epoch: 55/100... Step: 4590... Loss: 1.2968... Val Loss: 1.2746\n",
      "Epoch: 55/100... Step: 4600... Loss: 1.3183... Val Loss: 1.2724\n",
      "Epoch: 55/100... Step: 4610... Loss: 1.3052... Val Loss: 1.2730\n",
      "Epoch: 55/100... Step: 4620... Loss: 1.3377... Val Loss: 1.2742\n",
      "Epoch: 56/100... Step: 4630... Loss: 1.3105... Val Loss: 1.2739\n",
      "Epoch: 56/100... Step: 4640... Loss: 1.3051... Val Loss: 1.2723\n",
      "Epoch: 56/100... Step: 4650... Loss: 1.2918... Val Loss: 1.2711\n",
      "Epoch: 56/100... Step: 4660... Loss: 1.3215... Val Loss: 1.2725\n",
      "Epoch: 56/100... Step: 4670... Loss: 1.3131... Val Loss: 1.2727\n",
      "Epoch: 56/100... Step: 4680... Loss: 1.3342... Val Loss: 1.2742\n",
      "Epoch: 56/100... Step: 4690... Loss: 1.2922... Val Loss: 1.2705\n",
      "Epoch: 56/100... Step: 4700... Loss: 1.3134... Val Loss: 1.2704\n",
      "Epoch: 57/100... Step: 4710... Loss: 1.3199... Val Loss: 1.2697\n",
      "Epoch: 57/100... Step: 4720... Loss: 1.3058... Val Loss: 1.2715\n",
      "Epoch: 57/100... Step: 4730... Loss: 1.3094... Val Loss: 1.2719\n",
      "Epoch: 57/100... Step: 4740... Loss: 1.3182... Val Loss: 1.2721\n",
      "Epoch: 57/100... Step: 4750... Loss: 1.2812... Val Loss: 1.2702\n",
      "Epoch: 57/100... Step: 4760... Loss: 1.3000... Val Loss: 1.2696\n",
      "Epoch: 57/100... Step: 4770... Loss: 1.2953... Val Loss: 1.2712\n",
      "Epoch: 57/100... Step: 4780... Loss: 1.2882... Val Loss: 1.2707\n",
      "Epoch: 58/100... Step: 4790... Loss: 1.2853... Val Loss: 1.2701\n",
      "Epoch: 58/100... Step: 4800... Loss: 1.2877... Val Loss: 1.2704\n",
      "Epoch: 58/100... Step: 4810... Loss: 1.2857... Val Loss: 1.2682\n",
      "Epoch: 58/100... Step: 4820... Loss: 1.3126... Val Loss: 1.2675\n",
      "Epoch: 58/100... Step: 4830... Loss: 1.3121... Val Loss: 1.2719\n",
      "Epoch: 58/100... Step: 4840... Loss: 1.3058... Val Loss: 1.2699\n",
      "Epoch: 58/100... Step: 4850... Loss: 1.3057... Val Loss: 1.2678\n",
      "Epoch: 58/100... Step: 4860... Loss: 1.3034... Val Loss: 1.2670\n",
      "Epoch: 58/100... Step: 4870... Loss: 1.2867... Val Loss: 1.2660\n",
      "Epoch: 59/100... Step: 4880... Loss: 1.3056... Val Loss: 1.2662\n",
      "Epoch: 59/100... Step: 4890... Loss: 1.2969... Val Loss: 1.2663\n",
      "Epoch: 59/100... Step: 4900... Loss: 1.3036... Val Loss: 1.2657\n",
      "Epoch: 59/100... Step: 4910... Loss: 1.2773... Val Loss: 1.2648\n",
      "Epoch: 59/100... Step: 4920... Loss: 1.2775... Val Loss: 1.2641\n",
      "Epoch: 59/100... Step: 4930... Loss: 1.2779... Val Loss: 1.2646\n",
      "Epoch: 59/100... Step: 4940... Loss: 1.2698... Val Loss: 1.2691\n",
      "Epoch: 59/100... Step: 4950... Loss: 1.3123... Val Loss: 1.2651\n",
      "Epoch: 60/100... Step: 4960... Loss: 1.3129... Val Loss: 1.2654\n",
      "Epoch: 60/100... Step: 4970... Loss: 1.2787... Val Loss: 1.2663\n",
      "Epoch: 60/100... Step: 4980... Loss: 1.2677... Val Loss: 1.2656\n",
      "Epoch: 60/100... Step: 4990... Loss: 1.2758... Val Loss: 1.2649\n",
      "Epoch: 60/100... Step: 5000... Loss: 1.2957... Val Loss: 1.2646\n",
      "Epoch: 60/100... Step: 5010... Loss: 1.2835... Val Loss: 1.2649\n",
      "Epoch: 60/100... Step: 5020... Loss: 1.2951... Val Loss: 1.2626\n",
      "Epoch: 60/100... Step: 5030... Loss: 1.2844... Val Loss: 1.2613\n",
      "Epoch: 60/100... Step: 5040... Loss: 1.3143... Val Loss: 1.2639\n",
      "Epoch: 61/100... Step: 5050... Loss: 1.2889... Val Loss: 1.2628\n",
      "Epoch: 61/100... Step: 5060... Loss: 1.2783... Val Loss: 1.2607\n",
      "Epoch: 61/100... Step: 5070... Loss: 1.2666... Val Loss: 1.2622\n",
      "Epoch: 61/100... Step: 5080... Loss: 1.2923... Val Loss: 1.2611\n",
      "Epoch: 61/100... Step: 5090... Loss: 1.2952... Val Loss: 1.2636\n",
      "Epoch: 61/100... Step: 5100... Loss: 1.3155... Val Loss: 1.2642\n",
      "Epoch: 61/100... Step: 5110... Loss: 1.2763... Val Loss: 1.2617\n",
      "Epoch: 61/100... Step: 5120... Loss: 1.2936... Val Loss: 1.2602\n",
      "Epoch: 62/100... Step: 5130... Loss: 1.3010... Val Loss: 1.2601\n",
      "Epoch: 62/100... Step: 5140... Loss: 1.2795... Val Loss: 1.2626\n",
      "Epoch: 62/100... Step: 5150... Loss: 1.2932... Val Loss: 1.2610\n",
      "Epoch: 62/100... Step: 5160... Loss: 1.3030... Val Loss: 1.2607\n",
      "Epoch: 62/100... Step: 5170... Loss: 1.2614... Val Loss: 1.2591\n",
      "Epoch: 62/100... Step: 5180... Loss: 1.2846... Val Loss: 1.2597\n",
      "Epoch: 62/100... Step: 5190... Loss: 1.2793... Val Loss: 1.2601\n",
      "Epoch: 62/100... Step: 5200... Loss: 1.2701... Val Loss: 1.2617\n",
      "Epoch: 63/100... Step: 5210... Loss: 1.2666... Val Loss: 1.2601\n",
      "Epoch: 63/100... Step: 5220... Loss: 1.2719... Val Loss: 1.2589\n",
      "Epoch: 63/100... Step: 5230... Loss: 1.2650... Val Loss: 1.2577\n",
      "Epoch: 63/100... Step: 5240... Loss: 1.2996... Val Loss: 1.2570\n",
      "Epoch: 63/100... Step: 5250... Loss: 1.2979... Val Loss: 1.2601\n",
      "Epoch: 63/100... Step: 5260... Loss: 1.2856... Val Loss: 1.2604\n",
      "Epoch: 63/100... Step: 5270... Loss: 1.2860... Val Loss: 1.2577\n",
      "Epoch: 63/100... Step: 5280... Loss: 1.2823... Val Loss: 1.2563\n",
      "Epoch: 63/100... Step: 5290... Loss: 1.2704... Val Loss: 1.2561\n",
      "Epoch: 64/100... Step: 5300... Loss: 1.2846... Val Loss: 1.2566\n",
      "Epoch: 64/100... Step: 5310... Loss: 1.2797... Val Loss: 1.2564\n",
      "Epoch: 64/100... Step: 5320... Loss: 1.2894... Val Loss: 1.2573\n",
      "Epoch: 64/100... Step: 5330... Loss: 1.2644... Val Loss: 1.2551\n",
      "Epoch: 64/100... Step: 5340... Loss: 1.2565... Val Loss: 1.2546\n",
      "Epoch: 64/100... Step: 5350... Loss: 1.2703... Val Loss: 1.2567\n",
      "Epoch: 64/100... Step: 5360... Loss: 1.2579... Val Loss: 1.2588\n",
      "Epoch: 64/100... Step: 5370... Loss: 1.2946... Val Loss: 1.2581\n",
      "Epoch: 65/100... Step: 5380... Loss: 1.2984... Val Loss: 1.2565\n",
      "Epoch: 65/100... Step: 5390... Loss: 1.2632... Val Loss: 1.2571\n",
      "Epoch: 65/100... Step: 5400... Loss: 1.2616... Val Loss: 1.2586\n",
      "Epoch: 65/100... Step: 5410... Loss: 1.2527... Val Loss: 1.2568\n",
      "Epoch: 65/100... Step: 5420... Loss: 1.2791... Val Loss: 1.2581\n",
      "Epoch: 65/100... Step: 5430... Loss: 1.2640... Val Loss: 1.2571\n",
      "Epoch: 65/100... Step: 5440... Loss: 1.2821... Val Loss: 1.2542\n",
      "Epoch: 65/100... Step: 5450... Loss: 1.2657... Val Loss: 1.2527\n",
      "Epoch: 65/100... Step: 5460... Loss: 1.3037... Val Loss: 1.2556\n",
      "Epoch: 66/100... Step: 5470... Loss: 1.2766... Val Loss: 1.2548\n",
      "Epoch: 66/100... Step: 5480... Loss: 1.2660... Val Loss: 1.2531\n",
      "Epoch: 66/100... Step: 5490... Loss: 1.2482... Val Loss: 1.2531\n",
      "Epoch: 66/100... Step: 5500... Loss: 1.2715... Val Loss: 1.2532\n",
      "Epoch: 66/100... Step: 5510... Loss: 1.2767... Val Loss: 1.2548\n",
      "Epoch: 66/100... Step: 5520... Loss: 1.3069... Val Loss: 1.2564\n",
      "Epoch: 66/100... Step: 5530... Loss: 1.2528... Val Loss: 1.2551\n",
      "Epoch: 66/100... Step: 5540... Loss: 1.2757... Val Loss: 1.2526\n",
      "Epoch: 67/100... Step: 5550... Loss: 1.2798... Val Loss: 1.2519\n",
      "Epoch: 67/100... Step: 5560... Loss: 1.2608... Val Loss: 1.2546\n",
      "Epoch: 67/100... Step: 5570... Loss: 1.2790... Val Loss: 1.2534\n",
      "Epoch: 67/100... Step: 5580... Loss: 1.2908... Val Loss: 1.2532\n",
      "Epoch: 67/100... Step: 5590... Loss: 1.2512... Val Loss: 1.2516\n",
      "Epoch: 67/100... Step: 5600... Loss: 1.2683... Val Loss: 1.2528\n",
      "Epoch: 67/100... Step: 5610... Loss: 1.2595... Val Loss: 1.2534\n",
      "Epoch: 67/100... Step: 5620... Loss: 1.2580... Val Loss: 1.2535\n",
      "Epoch: 68/100... Step: 5630... Loss: 1.2546... Val Loss: 1.2539\n",
      "Epoch: 68/100... Step: 5640... Loss: 1.2560... Val Loss: 1.2520\n",
      "Epoch: 68/100... Step: 5650... Loss: 1.2555... Val Loss: 1.2497\n",
      "Epoch: 68/100... Step: 5660... Loss: 1.2912... Val Loss: 1.2505\n",
      "Epoch: 68/100... Step: 5670... Loss: 1.2810... Val Loss: 1.2507\n",
      "Epoch: 68/100... Step: 5680... Loss: 1.2703... Val Loss: 1.2527\n",
      "Epoch: 68/100... Step: 5690... Loss: 1.2676... Val Loss: 1.2509\n",
      "Epoch: 68/100... Step: 5700... Loss: 1.2648... Val Loss: 1.2489\n",
      "Epoch: 68/100... Step: 5710... Loss: 1.2498... Val Loss: 1.2491\n",
      "Epoch: 69/100... Step: 5720... Loss: 1.2672... Val Loss: 1.2506\n",
      "Epoch: 69/100... Step: 5730... Loss: 1.2614... Val Loss: 1.2508\n",
      "Epoch: 69/100... Step: 5740... Loss: 1.2669... Val Loss: 1.2489\n",
      "Epoch: 69/100... Step: 5750... Loss: 1.2454... Val Loss: 1.2487\n",
      "Epoch: 69/100... Step: 5760... Loss: 1.2454... Val Loss: 1.2476\n",
      "Epoch: 69/100... Step: 5770... Loss: 1.2461... Val Loss: 1.2478\n",
      "Epoch: 69/100... Step: 5780... Loss: 1.2400... Val Loss: 1.2519\n",
      "Epoch: 69/100... Step: 5790... Loss: 1.2816... Val Loss: 1.2515\n",
      "Epoch: 70/100... Step: 5800... Loss: 1.2813... Val Loss: 1.2491\n",
      "Epoch: 70/100... Step: 5810... Loss: 1.2439... Val Loss: 1.2485\n",
      "Epoch: 70/100... Step: 5820... Loss: 1.2476... Val Loss: 1.2477\n",
      "Epoch: 70/100... Step: 5830... Loss: 1.2412... Val Loss: 1.2500\n",
      "Epoch: 70/100... Step: 5840... Loss: 1.2643... Val Loss: 1.2494\n",
      "Epoch: 70/100... Step: 5850... Loss: 1.2443... Val Loss: 1.2487\n",
      "Epoch: 70/100... Step: 5860... Loss: 1.2686... Val Loss: 1.2466\n",
      "Epoch: 70/100... Step: 5870... Loss: 1.2536... Val Loss: 1.2457\n",
      "Epoch: 70/100... Step: 5880... Loss: 1.2870... Val Loss: 1.2460\n",
      "Epoch: 71/100... Step: 5890... Loss: 1.2575... Val Loss: 1.2483\n",
      "Epoch: 71/100... Step: 5900... Loss: 1.2539... Val Loss: 1.2460\n",
      "Epoch: 71/100... Step: 5910... Loss: 1.2357... Val Loss: 1.2452\n",
      "Epoch: 71/100... Step: 5920... Loss: 1.2616... Val Loss: 1.2456\n",
      "Epoch: 71/100... Step: 5930... Loss: 1.2587... Val Loss: 1.2469\n",
      "Epoch: 71/100... Step: 5940... Loss: 1.2847... Val Loss: 1.2471\n",
      "Epoch: 71/100... Step: 5950... Loss: 1.2401... Val Loss: 1.2488\n",
      "Epoch: 71/100... Step: 5960... Loss: 1.2549... Val Loss: 1.2465\n",
      "Epoch: 72/100... Step: 5970... Loss: 1.2730... Val Loss: 1.2458\n",
      "Epoch: 72/100... Step: 5980... Loss: 1.2565... Val Loss: 1.2467\n",
      "Epoch: 72/100... Step: 5990... Loss: 1.2638... Val Loss: 1.2483\n",
      "Epoch: 72/100... Step: 6000... Loss: 1.2693... Val Loss: 1.2477\n",
      "Epoch: 72/100... Step: 6010... Loss: 1.2339... Val Loss: 1.2453\n",
      "Epoch: 72/100... Step: 6020... Loss: 1.2465... Val Loss: 1.2457\n",
      "Epoch: 72/100... Step: 6030... Loss: 1.2421... Val Loss: 1.2457\n",
      "Epoch: 72/100... Step: 6040... Loss: 1.2431... Val Loss: 1.2448\n",
      "Epoch: 73/100... Step: 6050... Loss: 1.2338... Val Loss: 1.2491\n",
      "Epoch: 73/100... Step: 6060... Loss: 1.2477... Val Loss: 1.2462\n",
      "Epoch: 73/100... Step: 6070... Loss: 1.2438... Val Loss: 1.2426\n",
      "Epoch: 73/100... Step: 6080... Loss: 1.2713... Val Loss: 1.2434\n",
      "Epoch: 73/100... Step: 6090... Loss: 1.2622... Val Loss: 1.2473\n",
      "Epoch: 73/100... Step: 6100... Loss: 1.2563... Val Loss: 1.2469\n",
      "Epoch: 73/100... Step: 6110... Loss: 1.2553... Val Loss: 1.2451\n",
      "Epoch: 73/100... Step: 6120... Loss: 1.2565... Val Loss: 1.2430\n",
      "Epoch: 73/100... Step: 6130... Loss: 1.2352... Val Loss: 1.2433\n",
      "Epoch: 74/100... Step: 6140... Loss: 1.2531... Val Loss: 1.2428\n",
      "Epoch: 74/100... Step: 6150... Loss: 1.2462... Val Loss: 1.2451\n",
      "Epoch: 74/100... Step: 6160... Loss: 1.2552... Val Loss: 1.2444\n",
      "Epoch: 74/100... Step: 6170... Loss: 1.2350... Val Loss: 1.2438\n",
      "Epoch: 74/100... Step: 6180... Loss: 1.2294... Val Loss: 1.2428\n",
      "Epoch: 74/100... Step: 6190... Loss: 1.2332... Val Loss: 1.2443\n",
      "Epoch: 74/100... Step: 6200... Loss: 1.2262... Val Loss: 1.2462\n",
      "Epoch: 74/100... Step: 6210... Loss: 1.2683... Val Loss: 1.2462\n",
      "Epoch: 75/100... Step: 6220... Loss: 1.2668... Val Loss: 1.2431\n",
      "Epoch: 75/100... Step: 6230... Loss: 1.2332... Val Loss: 1.2428\n",
      "Epoch: 75/100... Step: 6240... Loss: 1.2305... Val Loss: 1.2410\n",
      "Epoch: 75/100... Step: 6250... Loss: 1.2304... Val Loss: 1.2449\n",
      "Epoch: 75/100... Step: 6260... Loss: 1.2519... Val Loss: 1.2465\n",
      "Epoch: 75/100... Step: 6270... Loss: 1.2364... Val Loss: 1.2449\n",
      "Epoch: 75/100... Step: 6280... Loss: 1.2527... Val Loss: 1.2410\n",
      "Epoch: 75/100... Step: 6290... Loss: 1.2395... Val Loss: 1.2405\n",
      "Epoch: 75/100... Step: 6300... Loss: 1.2655... Val Loss: 1.2418\n",
      "Epoch: 76/100... Step: 6310... Loss: 1.2436... Val Loss: 1.2424\n",
      "Epoch: 76/100... Step: 6320... Loss: 1.2456... Val Loss: 1.2413\n",
      "Epoch: 76/100... Step: 6330... Loss: 1.2270... Val Loss: 1.2395\n",
      "Epoch: 76/100... Step: 6340... Loss: 1.2534... Val Loss: 1.2406\n",
      "Epoch: 76/100... Step: 6350... Loss: 1.2497... Val Loss: 1.2398\n",
      "Epoch: 76/100... Step: 6360... Loss: 1.2711... Val Loss: 1.2433\n",
      "Epoch: 76/100... Step: 6370... Loss: 1.2259... Val Loss: 1.2442\n",
      "Epoch: 76/100... Step: 6380... Loss: 1.2531... Val Loss: 1.2428\n",
      "Epoch: 77/100... Step: 6390... Loss: 1.2604... Val Loss: 1.2405\n",
      "Epoch: 77/100... Step: 6400... Loss: 1.2400... Val Loss: 1.2406\n",
      "Epoch: 77/100... Step: 6410... Loss: 1.2530... Val Loss: 1.2415\n",
      "Epoch: 77/100... Step: 6420... Loss: 1.2567... Val Loss: 1.2436\n",
      "Epoch: 77/100... Step: 6430... Loss: 1.2290... Val Loss: 1.2414\n",
      "Epoch: 77/100... Step: 6440... Loss: 1.2331... Val Loss: 1.2403\n",
      "Epoch: 77/100... Step: 6450... Loss: 1.2358... Val Loss: 1.2381\n",
      "Epoch: 77/100... Step: 6460... Loss: 1.2334... Val Loss: 1.2377\n",
      "Epoch: 78/100... Step: 6470... Loss: 1.2271... Val Loss: 1.2406\n",
      "Epoch: 78/100... Step: 6480... Loss: 1.2389... Val Loss: 1.2444\n",
      "Epoch: 78/100... Step: 6490... Loss: 1.2236... Val Loss: 1.2401\n",
      "Epoch: 78/100... Step: 6500... Loss: 1.2617... Val Loss: 1.2380\n",
      "Epoch: 78/100... Step: 6510... Loss: 1.2523... Val Loss: 1.2392\n",
      "Epoch: 78/100... Step: 6520... Loss: 1.2528... Val Loss: 1.2418\n",
      "Epoch: 78/100... Step: 6530... Loss: 1.2428... Val Loss: 1.2419\n",
      "Epoch: 78/100... Step: 6540... Loss: 1.2452... Val Loss: 1.2397\n",
      "Epoch: 78/100... Step: 6550... Loss: 1.2269... Val Loss: 1.2383\n",
      "Epoch: 79/100... Step: 6560... Loss: 1.2446... Val Loss: 1.2379\n",
      "Epoch: 79/100... Step: 6570... Loss: 1.2374... Val Loss: 1.2385\n",
      "Epoch: 79/100... Step: 6580... Loss: 1.2454... Val Loss: 1.2391\n",
      "Epoch: 79/100... Step: 6590... Loss: 1.2207... Val Loss: 1.2393\n",
      "Epoch: 79/100... Step: 6600... Loss: 1.2199... Val Loss: 1.2380\n",
      "Epoch: 79/100... Step: 6610... Loss: 1.2228... Val Loss: 1.2380\n",
      "Epoch: 79/100... Step: 6620... Loss: 1.2161... Val Loss: 1.2378\n",
      "Epoch: 79/100... Step: 6630... Loss: 1.2531... Val Loss: 1.2396\n",
      "Epoch: 80/100... Step: 6640... Loss: 1.2492... Val Loss: 1.2416\n",
      "Epoch: 80/100... Step: 6650... Loss: 1.2155... Val Loss: 1.2427\n",
      "Epoch: 80/100... Step: 6660... Loss: 1.2147... Val Loss: 1.2385\n",
      "Epoch: 80/100... Step: 6670... Loss: 1.2122... Val Loss: 1.2371\n",
      "Epoch: 80/100... Step: 6680... Loss: 1.2364... Val Loss: 1.2389\n",
      "Epoch: 80/100... Step: 6690... Loss: 1.2293... Val Loss: 1.2412\n",
      "Epoch: 80/100... Step: 6700... Loss: 1.2461... Val Loss: 1.2388\n",
      "Epoch: 80/100... Step: 6710... Loss: 1.2253... Val Loss: 1.2351\n",
      "Epoch: 80/100... Step: 6720... Loss: 1.2549... Val Loss: 1.2360\n",
      "Epoch: 81/100... Step: 6730... Loss: 1.2359... Val Loss: 1.2354\n",
      "Epoch: 81/100... Step: 6740... Loss: 1.2268... Val Loss: 1.2360\n",
      "Epoch: 81/100... Step: 6750... Loss: 1.2136... Val Loss: 1.2374\n",
      "Epoch: 81/100... Step: 6760... Loss: 1.2398... Val Loss: 1.2371\n",
      "Epoch: 81/100... Step: 6770... Loss: 1.2341... Val Loss: 1.2351\n",
      "Epoch: 81/100... Step: 6780... Loss: 1.2573... Val Loss: 1.2364\n",
      "Epoch: 81/100... Step: 6790... Loss: 1.2154... Val Loss: 1.2387\n",
      "Epoch: 81/100... Step: 6800... Loss: 1.2379... Val Loss: 1.2396\n",
      "Epoch: 82/100... Step: 6810... Loss: 1.2429... Val Loss: 1.2388\n",
      "Epoch: 82/100... Step: 6820... Loss: 1.2209... Val Loss: 1.2382\n",
      "Epoch: 82/100... Step: 6830... Loss: 1.2367... Val Loss: 1.2355\n",
      "Epoch: 82/100... Step: 6840... Loss: 1.2444... Val Loss: 1.2367\n",
      "Epoch: 82/100... Step: 6850... Loss: 1.2042... Val Loss: 1.2356\n",
      "Epoch: 82/100... Step: 6860... Loss: 1.2228... Val Loss: 1.2364\n",
      "Epoch: 82/100... Step: 6870... Loss: 1.2182... Val Loss: 1.2352\n",
      "Epoch: 82/100... Step: 6880... Loss: 1.2207... Val Loss: 1.2326\n",
      "Epoch: 83/100... Step: 6890... Loss: 1.2214... Val Loss: 1.2356\n",
      "Epoch: 83/100... Step: 6900... Loss: 1.2155... Val Loss: 1.2377\n",
      "Epoch: 83/100... Step: 6910... Loss: 1.2104... Val Loss: 1.2361\n",
      "Epoch: 83/100... Step: 6920... Loss: 1.2540... Val Loss: 1.2342\n",
      "Epoch: 83/100... Step: 6930... Loss: 1.2402... Val Loss: 1.2365\n",
      "Epoch: 83/100... Step: 6940... Loss: 1.2306... Val Loss: 1.2373\n",
      "Epoch: 83/100... Step: 6950... Loss: 1.2320... Val Loss: 1.2379\n",
      "Epoch: 83/100... Step: 6960... Loss: 1.2332... Val Loss: 1.2371\n",
      "Epoch: 83/100... Step: 6970... Loss: 1.2118... Val Loss: 1.2345\n",
      "Epoch: 84/100... Step: 6980... Loss: 1.2304... Val Loss: 1.2342\n",
      "Epoch: 84/100... Step: 6990... Loss: 1.2301... Val Loss: 1.2356\n",
      "Epoch: 84/100... Step: 7000... Loss: 1.2338... Val Loss: 1.2356\n",
      "Epoch: 84/100... Step: 7010... Loss: 1.2122... Val Loss: 1.2367\n",
      "Epoch: 84/100... Step: 7020... Loss: 1.2116... Val Loss: 1.2338\n",
      "Epoch: 84/100... Step: 7030... Loss: 1.2090... Val Loss: 1.2340\n",
      "Epoch: 84/100... Step: 7040... Loss: 1.2050... Val Loss: 1.2338\n",
      "Epoch: 84/100... Step: 7050... Loss: 1.2427... Val Loss: 1.2357\n",
      "Epoch: 85/100... Step: 7060... Loss: 1.2383... Val Loss: 1.2365\n",
      "Epoch: 85/100... Step: 7070... Loss: 1.2086... Val Loss: 1.2386\n",
      "Epoch: 85/100... Step: 7080... Loss: 1.2053... Val Loss: 1.2366\n",
      "Epoch: 85/100... Step: 7090... Loss: 1.1978... Val Loss: 1.2332\n",
      "Epoch: 85/100... Step: 7100... Loss: 1.2239... Val Loss: 1.2356\n",
      "Epoch: 85/100... Step: 7110... Loss: 1.2210... Val Loss: 1.2375\n",
      "Epoch: 85/100... Step: 7120... Loss: 1.2312... Val Loss: 1.2358\n",
      "Epoch: 85/100... Step: 7130... Loss: 1.2099... Val Loss: 1.2323\n",
      "Epoch: 85/100... Step: 7140... Loss: 1.2492... Val Loss: 1.2310\n",
      "Epoch: 86/100... Step: 7150... Loss: 1.2274... Val Loss: 1.2317\n",
      "Epoch: 86/100... Step: 7160... Loss: 1.2185... Val Loss: 1.2333\n",
      "Epoch: 86/100... Step: 7170... Loss: 1.2008... Val Loss: 1.2349\n",
      "Epoch: 86/100... Step: 7180... Loss: 1.2260... Val Loss: 1.2348\n",
      "Epoch: 86/100... Step: 7190... Loss: 1.2218... Val Loss: 1.2347\n",
      "Epoch: 86/100... Step: 7200... Loss: 1.2474... Val Loss: 1.2346\n",
      "Epoch: 86/100... Step: 7210... Loss: 1.2011... Val Loss: 1.2357\n",
      "Epoch: 86/100... Step: 7220... Loss: 1.2245... Val Loss: 1.2364\n",
      "Epoch: 87/100... Step: 7230... Loss: 1.2323... Val Loss: 1.2365\n",
      "Epoch: 87/100... Step: 7240... Loss: 1.2096... Val Loss: 1.2353\n",
      "Epoch: 87/100... Step: 7250... Loss: 1.2221... Val Loss: 1.2336\n",
      "Epoch: 87/100... Step: 7260... Loss: 1.2294... Val Loss: 1.2335\n",
      "Epoch: 87/100... Step: 7270... Loss: 1.2028... Val Loss: 1.2338\n",
      "Epoch: 87/100... Step: 7280... Loss: 1.2148... Val Loss: 1.2343\n",
      "Epoch: 87/100... Step: 7290... Loss: 1.2162... Val Loss: 1.2325\n",
      "Epoch: 87/100... Step: 7300... Loss: 1.2095... Val Loss: 1.2311\n",
      "Epoch: 88/100... Step: 7310... Loss: 1.2038... Val Loss: 1.2327\n",
      "Epoch: 88/100... Step: 7320... Loss: 1.1997... Val Loss: 1.2344\n",
      "Epoch: 88/100... Step: 7330... Loss: 1.1996... Val Loss: 1.2327\n",
      "Epoch: 88/100... Step: 7340... Loss: 1.2423... Val Loss: 1.2324\n",
      "Epoch: 88/100... Step: 7350... Loss: 1.2295... Val Loss: 1.2336\n",
      "Epoch: 88/100... Step: 7360... Loss: 1.2191... Val Loss: 1.2336\n",
      "Epoch: 88/100... Step: 7370... Loss: 1.2220... Val Loss: 1.2346\n",
      "Epoch: 88/100... Step: 7380... Loss: 1.2191... Val Loss: 1.2348\n",
      "Epoch: 88/100... Step: 7390... Loss: 1.2075... Val Loss: 1.2323\n",
      "Epoch: 89/100... Step: 7400... Loss: 1.2155... Val Loss: 1.2302\n",
      "Epoch: 89/100... Step: 7410... Loss: 1.2267... Val Loss: 1.2317\n",
      "Epoch: 89/100... Step: 7420... Loss: 1.2185... Val Loss: 1.2302\n",
      "Epoch: 89/100... Step: 7430... Loss: 1.1962... Val Loss: 1.2317\n",
      "Epoch: 89/100... Step: 7440... Loss: 1.1946... Val Loss: 1.2304\n",
      "Epoch: 89/100... Step: 7450... Loss: 1.2030... Val Loss: 1.2310\n",
      "Epoch: 89/100... Step: 7460... Loss: 1.1942... Val Loss: 1.2305\n",
      "Epoch: 89/100... Step: 7470... Loss: 1.2400... Val Loss: 1.2302\n",
      "Epoch: 90/100... Step: 7480... Loss: 1.2324... Val Loss: 1.2329\n",
      "Epoch: 90/100... Step: 7490... Loss: 1.1984... Val Loss: 1.2368\n",
      "Epoch: 90/100... Step: 7500... Loss: 1.1916... Val Loss: 1.2345\n",
      "Epoch: 90/100... Step: 7510... Loss: 1.1927... Val Loss: 1.2313\n",
      "Epoch: 90/100... Step: 7520... Loss: 1.2183... Val Loss: 1.2323\n",
      "Epoch: 90/100... Step: 7530... Loss: 1.2128... Val Loss: 1.2342\n",
      "Epoch: 90/100... Step: 7540... Loss: 1.2160... Val Loss: 1.2330\n",
      "Epoch: 90/100... Step: 7550... Loss: 1.2018... Val Loss: 1.2297\n",
      "Epoch: 90/100... Step: 7560... Loss: 1.2330... Val Loss: 1.2287\n",
      "Epoch: 91/100... Step: 7570... Loss: 1.2145... Val Loss: 1.2283\n",
      "Epoch: 91/100... Step: 7580... Loss: 1.2131... Val Loss: 1.2294\n",
      "Epoch: 91/100... Step: 7590... Loss: 1.1891... Val Loss: 1.2305\n",
      "Epoch: 91/100... Step: 7600... Loss: 1.2135... Val Loss: 1.2310\n",
      "Epoch: 91/100... Step: 7610... Loss: 1.2141... Val Loss: 1.2301\n",
      "Epoch: 91/100... Step: 7620... Loss: 1.2301... Val Loss: 1.2299\n",
      "Epoch: 91/100... Step: 7630... Loss: 1.1903... Val Loss: 1.2316\n",
      "Epoch: 91/100... Step: 7640... Loss: 1.2078... Val Loss: 1.2333\n",
      "Epoch: 92/100... Step: 7650... Loss: 1.2143... Val Loss: 1.2334\n",
      "Epoch: 92/100... Step: 7660... Loss: 1.2020... Val Loss: 1.2312\n",
      "Epoch: 92/100... Step: 7670... Loss: 1.2080... Val Loss: 1.2324\n",
      "Epoch: 92/100... Step: 7680... Loss: 1.2254... Val Loss: 1.2320\n",
      "Epoch: 92/100... Step: 7690... Loss: 1.1886... Val Loss: 1.2305\n",
      "Epoch: 92/100... Step: 7700... Loss: 1.2025... Val Loss: 1.2325\n",
      "Epoch: 92/100... Step: 7710... Loss: 1.2079... Val Loss: 1.2300\n",
      "Epoch: 92/100... Step: 7720... Loss: 1.1910... Val Loss: 1.2271\n",
      "Epoch: 93/100... Step: 7730... Loss: 1.1916... Val Loss: 1.2294\n",
      "Epoch: 93/100... Step: 7740... Loss: 1.1956... Val Loss: 1.2302\n",
      "Epoch: 93/100... Step: 7750... Loss: 1.1942... Val Loss: 1.2313\n",
      "Epoch: 93/100... Step: 7760... Loss: 1.2272... Val Loss: 1.2313\n",
      "Epoch: 93/100... Step: 7770... Loss: 1.2236... Val Loss: 1.2327\n",
      "Epoch: 93/100... Step: 7780... Loss: 1.2142... Val Loss: 1.2306\n",
      "Epoch: 93/100... Step: 7790... Loss: 1.2176... Val Loss: 1.2299\n",
      "Epoch: 93/100... Step: 7800... Loss: 1.2100... Val Loss: 1.2302\n",
      "Epoch: 93/100... Step: 7810... Loss: 1.1925... Val Loss: 1.2312\n",
      "Epoch: 94/100... Step: 7820... Loss: 1.2119... Val Loss: 1.2285\n",
      "Epoch: 94/100... Step: 7830... Loss: 1.2052... Val Loss: 1.2296\n",
      "Epoch: 94/100... Step: 7840... Loss: 1.2137... Val Loss: 1.2286\n",
      "Epoch: 94/100... Step: 7850... Loss: 1.1827... Val Loss: 1.2283\n",
      "Epoch: 94/100... Step: 7860... Loss: 1.1849... Val Loss: 1.2269\n",
      "Epoch: 94/100... Step: 7870... Loss: 1.1912... Val Loss: 1.2294\n",
      "Epoch: 94/100... Step: 7880... Loss: 1.1864... Val Loss: 1.2294\n",
      "Epoch: 94/100... Step: 7890... Loss: 1.2259... Val Loss: 1.2290\n",
      "Epoch: 95/100... Step: 7900... Loss: 1.2195... Val Loss: 1.2317\n",
      "Epoch: 95/100... Step: 7910... Loss: 1.1837... Val Loss: 1.2322\n",
      "Epoch: 95/100... Step: 7920... Loss: 1.1820... Val Loss: 1.2319\n",
      "Epoch: 95/100... Step: 7930... Loss: 1.1816... Val Loss: 1.2292\n",
      "Epoch: 95/100... Step: 7940... Loss: 1.2020... Val Loss: 1.2337\n",
      "Epoch: 95/100... Step: 7950... Loss: 1.1902... Val Loss: 1.2327\n",
      "Epoch: 95/100... Step: 7960... Loss: 1.2039... Val Loss: 1.2311\n",
      "Epoch: 95/100... Step: 7970... Loss: 1.1964... Val Loss: 1.2285\n",
      "Epoch: 95/100... Step: 7980... Loss: 1.2278... Val Loss: 1.2267\n",
      "Epoch: 96/100... Step: 7990... Loss: 1.2068... Val Loss: 1.2273\n",
      "Epoch: 96/100... Step: 8000... Loss: 1.2002... Val Loss: 1.2281\n",
      "Epoch: 96/100... Step: 8010... Loss: 1.1822... Val Loss: 1.2265\n",
      "Epoch: 96/100... Step: 8020... Loss: 1.2058... Val Loss: 1.2275\n",
      "Epoch: 96/100... Step: 8030... Loss: 1.2112... Val Loss: 1.2279\n",
      "Epoch: 96/100... Step: 8040... Loss: 1.2289... Val Loss: 1.2292\n",
      "Epoch: 96/100... Step: 8050... Loss: 1.1801... Val Loss: 1.2287\n",
      "Epoch: 96/100... Step: 8060... Loss: 1.2056... Val Loss: 1.2306\n",
      "Epoch: 97/100... Step: 8070... Loss: 1.2051... Val Loss: 1.2308\n",
      "Epoch: 97/100... Step: 8080... Loss: 1.1939... Val Loss: 1.2326\n",
      "Epoch: 97/100... Step: 8090... Loss: 1.2113... Val Loss: 1.2312\n",
      "Epoch: 97/100... Step: 8100... Loss: 1.2139... Val Loss: 1.2297\n",
      "Epoch: 97/100... Step: 8110... Loss: 1.1807... Val Loss: 1.2285\n",
      "Epoch: 97/100... Step: 8120... Loss: 1.1970... Val Loss: 1.2329\n",
      "Epoch: 97/100... Step: 8130... Loss: 1.1945... Val Loss: 1.2303\n",
      "Epoch: 97/100... Step: 8140... Loss: 1.1865... Val Loss: 1.2260\n",
      "Epoch: 98/100... Step: 8150... Loss: 1.1851... Val Loss: 1.2265\n",
      "Epoch: 98/100... Step: 8160... Loss: 1.1942... Val Loss: 1.2266\n",
      "Epoch: 98/100... Step: 8170... Loss: 1.1889... Val Loss: 1.2274\n",
      "Epoch: 98/100... Step: 8180... Loss: 1.2207... Val Loss: 1.2272\n",
      "Epoch: 98/100... Step: 8190... Loss: 1.2199... Val Loss: 1.2286\n",
      "Epoch: 98/100... Step: 8200... Loss: 1.2007... Val Loss: 1.2266\n",
      "Epoch: 98/100... Step: 8210... Loss: 1.2035... Val Loss: 1.2268\n",
      "Epoch: 98/100... Step: 8220... Loss: 1.1990... Val Loss: 1.2270\n",
      "Epoch: 98/100... Step: 8230... Loss: 1.1806... Val Loss: 1.2289\n",
      "Epoch: 99/100... Step: 8240... Loss: 1.2012... Val Loss: 1.2272\n",
      "Epoch: 99/100... Step: 8250... Loss: 1.1970... Val Loss: 1.2273\n",
      "Epoch: 99/100... Step: 8260... Loss: 1.2070... Val Loss: 1.2284\n",
      "Epoch: 99/100... Step: 8270... Loss: 1.1783... Val Loss: 1.2273\n",
      "Epoch: 99/100... Step: 8280... Loss: 1.1768... Val Loss: 1.2248\n",
      "Epoch: 99/100... Step: 8290... Loss: 1.1816... Val Loss: 1.2263\n",
      "Epoch: 99/100... Step: 8300... Loss: 1.1751... Val Loss: 1.2282\n",
      "Epoch: 99/100... Step: 8310... Loss: 1.2064... Val Loss: 1.2270\n",
      "Epoch: 100/100... Step: 8320... Loss: 1.2168... Val Loss: 1.2278\n",
      "Epoch: 100/100... Step: 8330... Loss: 1.1721... Val Loss: 1.2296\n",
      "Epoch: 100/100... Step: 8340... Loss: 1.1750... Val Loss: 1.2285\n",
      "Epoch: 100/100... Step: 8350... Loss: 1.1715... Val Loss: 1.2291\n",
      "Epoch: 100/100... Step: 8360... Loss: 1.1970... Val Loss: 1.2309\n",
      "Epoch: 100/100... Step: 8370... Loss: 1.1839... Val Loss: 1.2287\n",
      "Epoch: 100/100... Step: 8380... Loss: 1.1996... Val Loss: 1.2282\n",
      "Epoch: 100/100... Step: 8390... Loss: 1.1834... Val Loss: 1.2254\n",
      "Epoch: 100/100... Step: 8400... Loss: 1.2231... Val Loss: 1.2251\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 160 \n",
    "n_epochs = 100 \n",
    "\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d2cc07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dante = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_dante, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3258fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() \n",
    "\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "925a9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='This', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() \n",
    "    \n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79344c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> What do you still be considered short from my paint? <E> <S> I have a gardance of a couration attach is well because I decided to go to a champoon <E> <S> I have a party to a brain taker, see about his finger. <E> <S> There are no more important, but I can buy a billion penny. <E> <S> When my wallet should be attending the shaming stop that we hope my foot said \"I don't know what me.\" <E> <S> I will see the worst minute a shitty pancake for some positive some of that book is the best way to could be saying that I can't get important as a bad poor. <E> <S> My friend told me to see my dreams of my back at single that I can't colorbly to a lot. <E> <S> The person who works, it would be taking the way your back taste on their panes <E> <S> A pedophile talks is a crowd of a little too stack to me. <E> <S> The opposite of a movie always been so many school all dog <E> <S> It was an all of the postman that my shampoo is the worst salad thing, there are a practice or them. <E> <S> I'm a socce and that it was a lot and says \"I'm so minute\" to and all tonight in a bad contest. <E> <S> It was a soliety welling service. He takes me batty. <E> <S> My girlfriend told me to stop trying to stop might be a star of a laser and started as moving. <E> <S> My wife told me to be anyone what Its so much of it. <E> <S> There's something the best thing to get texters without a bed, so I told him I was buying one. <E> <S> Without time we choose to get a shampoo <E> <S> I was stuck from the milk is the words that they always take myself over when I was stationery. <E> <S> I asked my four of the police with so time at home, she comes off the shower in. <E> <S> I have sex all more silence that are ask with the couple in my feet. <E> <S> I'd be handing my problem with the wedding though. <E> <S> My friend won't have a shit that has been a comebia too long and said, \"I don't know what he still thought you annoy all out of time. <E> <S> We saw a baby show on the stars today and I would stand to heard at the bird <E> <S> When I was in my wife I've been my favolite postman. That's a social one <E> <S> The way it defist for hard to convert a complex that said and seemed a perfect offender, but the best time to measure he can guy and a balance of the compacy. <E> <S> My wife was a brothel with a bunch, she says, it's a contest. <E> <S> If you told the whole world store to stay that all these people are looking at it and there are a sham one thing. <E> <S> I did the drug since that should be called that when I do not be traveling the sure my chest! <E> <S> There's a computer sentence that would spread my first time on my sinter. <E> <S> I told my toast because I'm good for a book at a balance and the pressure see the success <E> <S> A politician is so sales when the baserall thinks its a presture in this chop change. <E> <S> What says if I call it in those ties with a service of a broken stealing condiment, they are so left of a car, its a piete? <E> <S> The only pool install standing successfully be too late. <E> <S> I wore a paranoid that I wouldn't be the pane times and asked her to stop back in how telling me they weren't the star on the caladay. <E> <S> A child is a lifetime because I have a lot of carefully <E> <S> The whole life showed up to the week that I'll not conventable well and then hundor said it would go it a light. <E> <S> I've broke in money and the baseball has and a lot of people with them as a shop <E> <S> I don't know what a clack is starting to be a bad. <E> <S> My dad said that the same words a disease this was an universe inte problems, starting this side of the place. <E> <S> My friend said I have an alphabet support today but I had a pater. <E> <S> If you can tell it's serves than it's making pants about sex <E> <S> I didn't think that what I don't think I caut anywhere was a can back at me <E> <S> To all of an alliner back and a crushing dentist. <E> <S> And the word alphobers always be counter. <E> <S> If you can take a problem in his bank on yourself and the beep in me. <E> <S> My doctor says that I had a lot of confriction, but I was tolencauncy she doesn't want my freech. <E> <S> I would be at the sexy through an altist because I wonder in the world's barbage. <E> <S> I'm such the statue of a chipsens to told the whole winner where a constipation is losing the bear out of the place. <E> <S> I asked my wife to come over when it was at the shop well that he's a butterflies to her teeth! <E> <S> With a patering side and no louder then I went it with my bus but the original way it will be cannibally stronger while singer as they were at making a sinner to his first penis on my prison. <E> <S> It is into the best fatilus today is im on too long. <E> <S> I was so book at the morning I was a ped faster, but when the stoner is a losing shit of man to spell <E> <S> Most sitching is the best time to be a billoon tell. <E> <S> If you crying to get married, I cant get time to think that the other age. Well they should be an anger one to milk. <E> <S> If you're talking out that the person is to could say that it's a prive <E> <S> If anyone says it was off the place if a spant poor is the person to think I was to talk... <E> <S> To the story is a lot of memory the struggle of the prison on me. <E> <S> What do money takes marriages, it doesnt make you see the sided. <E> <S> Trump seems a better today; I winner if the only women were nothing to to buy and took an extra strong color after a camel. <E> <S> To all the book they will be is to be in cold. <E> <S> I wilived an exciler to try to take a pizza that I heard a conformation is to combine an alive. <E> <S> A bird wold is so born is a penis <E> <S> Wearing partner told me watch in so takes to say Ive got so society, I have a later only because all I would be all metalle. <E> <S> Treedy told me they are taken out of time to there shelf many times. <E> <S> I have not bought a parallel and winning it they... it still wants to be a board. <E> <S> When the propoline is won a book to be a straight one of money with all mornings, so I was the worst singer of people. <E> <S> I woke of a struckle tire and steal instruments of the click. <E> <S> The single place to make a strike and police change a step together. <E> <S> My family is mostly the price where I cancered the protect. <E> <S> If I do the way the person without his waist, it comes to this part. <E> <S> I was always laky and I was thinking about a morning to call the bottle of my food. <E> <S> It didn't see myself intiler it would be the second tent that tennis sentences working to a condem. <E> <S> If you think to make a more pants, then it war been this theory. <E> <S> If I decided to see my doctor that were all my sentences, I can be a perfect that so much and I've never seen that butt. <E> <S> My wife told me to be an empty way we could be a car on me. I think I have news to to the body there, I have but the person who have a shaped and I hurt the bars <E> <S> Who dread not to make it alone, because I didn't have anything or a door! <E> <S> Working in a bit of the single of an install popilation to the secret to the season <E> <S> If I don't like to take it, but and I wanted to change..... <E> <S> My wife told me I could say Ive got me so much better when it was a courage. <E> <S> A bar said and tells the minute we would sea that stable who had to shim the butt in my sentence. <E> <S> My friend said, \"I can die, but I'm getting mirrored.\" <E> <S> This polish weres a crop of a camp ass its the way to the world to see the callops. <E> <S> The butter of my son asked me it would be more left. <E> <S> I had terrible attending that the world touches to tole man stones in the sentence, but he hit me <E> <S> I was going to ball in too music we had a bad point <E> <S> I always be too. <E> <S> Without all of your foot support says I'm so pretty too the world. <E> <S> If I say Ive been thinking that the world's problem is so much of holes in sense of characters and should stop balk it who was a stop in tents. <E> <S> I'd get my son track and walking it out of some people. <E> <S> If anyone knows that I don't like to the sex in the best and they came in the way I took myself is a color of the place. <E> <S> My wife asked me if you want to have so much and the boxer will be too many doctors sells. <E> <S> I wonder if I don't succeed when I wish I didn't want to be in someone. <E> <S> Meda is the original thing, I didn't think about how many scales in. <E> <S> I'm not a luttering clock. <E> <S> I am going to give a silence but i was going to give a piece of shed those. <E> <S> If a difference wearing money is the boot or god when you sell the purp about the world <E> <S> I tried to get the difference between a struck but there has no themselves. <E> <S> A paranoid that came to a body who said it was the butt. <E> <S> My friends and my wife is the best thing about having a party words that I say Ive get more time to cheat their barkents at the best way it's such a paradox <E> <S> My best four band without a short book about the word that tennis and this surprose men in have you can't get the dog in me. <E> <S> My dictionary was going to come the word a book about someone without the part of the world. <E> <S> The supermarket is a bircon that was a car. <E> <S> I hate this toes at the winnor, they are better than the part. <E> <S> When a sex is the word \"peace of your time to start stolen\" <E> <S> If theres no one of this man to get a beer installed the book at the side if I can't spend as too tougher. <E> <S> When my friend is a candles, well, then we're all once that, the other depered me anywhere. <E> <S> Most men are so paranoid because they can tell your wife to spell anywory. <E> <S> I've been a problem with me when I would be a shit <E> <S> My dad said I'm genting a lot of people in the steam was all the thickers with the porn to the sense of the person to always help me as I win <E> <S> I did some pictures of politics and stings as\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 10000, prime='<S> ', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8889b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f80d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> I heard they're so political shaped and says in the can <E> <S> A man who don't take away with the world when you should see myself in the second. <E> <S> I think my girlfriend was too lot of and started to be so so trainers as a still, I think I went on her off an electricist. <E> <S> We all deal a book about the propesty of my wife. <E> <S> Whoever was the wailer, I was throwing me on my friends but now, they would be caught it. <E> <S> I wondered why the break and all the movie was a poor positive that was a short fall. <E> <S> I would spell short tennis, I'm starting to be anything alreidene. <E> <S> If an and person walks into a ball and thought I had how his water and the witch is when you worked to myself. <E> <S> I was going to tell you my grandfather, but it's been a bear but it was some proting on at them. <E> <S> To me sure, whats: as I saw, is a complete, bears the produster <E> <S> I take shoes and now the word answer to sell me her work, the brain is an alphabet. <E> <S> My favorite things are always trying to be changed trying to cross you the calitace. <E> <S> My wife and the shame is always being stocking and this women who want.. <E> <S> If the surgeon stole it in scheme? It was a share <E> <S> I'm getting a lanacing person, but I have a little beer at the street of something instead. <E> <S> Word from all of a musical password at a bad act and perfect sounds to too shop <E> <S> If a damn ass threes are the salty antidepreds and if you want more at the world? <E> <S> The best almasting to a braking peace and the person is the cross-attention to tell them it was a social old timer. <E> <S> There was on a different story to say what is it such a bull? <E> <S> The possible person who and this world is the best with taking order as they wouldn't have as taking a prostitute board. <E> <S> As i have a some computer when you were a book to said, \"Your man walks into a bar?\" <E> <S> The superwarder told me I dont have the world to stop at the crime to me. <E> <S> I have a big state, but that would spell pool shell. <E> <S> A man told me I couldnt ask a break and said she's staring too men who had a biggest thing about it <E> <S> I'm so but at a bad per with my wife I said \"Would a man, but when's the bad past is that's what a pood animals water in.\" <E> <S> I'm not a problem in their compater those, the worst way to can think I can always tell it. <E> <S> A moment is a bad clowhs, a chestary asked if you can't give the punch. <E> <S> I would be a cannibal if any cales on the past and then Im a bit of the people! <E> <S> Why is it see in the worst star, then I have to get married it was asshole? <E> <S> I wonder what my drigtter is strawering to the police on my stuck. <E> <S> I have an incresial price of the moon who sated the budden today, and I don't know what masturbation. <E> <S> All money couldnt get a common today <E> <S> My wife said that I was too late to say, I should have to start to say. <E> <S> In my court brower, it was a borel. <E> <S> Why do someone who could be a porn arm and not saying \"The people in the state of my car is better!\" <E> <S> A bitch told me that I should be an anticologies. He was cool when I'm getting a look at a lot of sheet. <E> <S> There's a business if you and my dick isn't a sentence of all these texts. <E> <S> I have a sex issues, its to go on my body. <E> <S> I have a person, I cant stand to think about a man who called him at me. <E> <S> If it works at a shoe areas and say I have sex things, we arent out of her singer. <E> <S> I was told that a people about my prophotous was there, in me, I was going to be a pencil <E> <S> I'm the same to the book over in the bus in the body <E> <S> I took a statement with a problem so many sense is shirt and it was there in the that to my wife I'm not made over my sight <E> <S> It's a long prefetition to take off in my birthday... <E> <S> I'd get a claim at my window, but there are sure to come out of a parator on their shit in the comedian. <E> <S> I went into a bar and the peace when my closes to say, but I was worried that I don't know what her art <E> <S> There's nothing to be an exishenting procrament of shaming in a course. <E> <S> I was willing to think I was supposed to be a barber, sounds to the poor with an apperies of solid. <E> <S> I have butterful my friends and saw and sells the closer because Im not suicing. <E> <S> I was always try and poleres. Thene would be a pick-it too. <E> <S> I heard the word man who should be called taking a start into a pretty gig on the paid. <E> <S> I have to stop asking me that I sen it off is technically. <E> <S> I always be a pooply tattoo <E> <S> I would make to the money in school, she said Im going to be a sole <E> <S> I tell my foot, all my wife was a balls. <E> <S> I didn't took it mistaken but it's not the bick of the part. <E> <S> I was so paid, I didn't like at a little only <E> <S> I wonder if I was thinking about my safe sex with a standand intervational actural banacal she were because the only shows won't be a bird. <E> <S> What does you call a sing as a practician song? <E> <S> Most mutist sucks is that they are leading figuring the best against the president. <E> <S> An anal asshile calls to the baseball could be antilist. <E> <S> My friends were told my whole way it's the bar then I can't stund as a condom, but how money. <E> <S> This car was a pretty person. The organizer is so much out of the way I to back it on a board as a cancel, but I've got these thats with any sticking sex instructor. <E> <S> All so many sex and they're too long. <E> <S> I was going to get the best farmer but then if you complate them, they're not a stationary. <E> <S> I wish it comes to the way to the world works and I have a cold farmer to be the best time. <E> <S> A bottle of action of that therapist was the sick of the memory... I have to be a last meat <E> <S> After I don't say that it was on the people on a lost thing anyone who said \"He says that I will never say \"Is a booby on more.\" <E> <S> An altal tries is a better stinky because its serurity than ten men <E> <S> My girlfriend said I have a bullet stolen standing in the world, I don't think I'm stealing. <E> <S> I hate a better famous some time I saw a some course on an one to me <E> <S> I was, someone still says Ive been surprise the therapy today a way to ask an impression is to be some people as I was a penis <E> <S> A bad train is taken to be the best fart in my cat. <E> <S> What do you know that you are porn when I say it is a paranoid? <E> <S> I am the bus but if I have to call it more than this song with an underwater. <E> <S> My friends are trying to change a book about batteries with the shit to be so start. <E> <S> As a burnithress why does you say were some prostitutes? <E> <S> When you see what you have sex, you're thrown for their stickers. <E> <S> If you can't have to spell things if you was she's lets about the person <E> <S> I wanted to be selling my couple in the world, but I dont have the brothels <E> <S> If I started a classical meta that had no one hears, then I see the cash to have a bad peace <E> <S> When I wanted to get my car after I saw a band into a calling to a camp, then the barkender had a carefully. <E> <S> I was a lot of calling the stupied of my son who causes me in my friend. <E> <S> It was the weirdest place to be too many times when I can buy the winner. <E> <S> I have become a bridger and I could say it was all short. <E> <S> I wonder if the one of the people are people that can say I dropped the world and I'm a prochoust for the bar and see it all on me. <E> <S> I tried to show the secret today, I should be a sex checkers. <E> <S> I watched home walking around the shower of my better all of a sea it said I who have a painting. <E> <S> My wife is a soun of myself....... It is a bit of the computer? <E> <S> I was watching the part if I was always to get the broundens. <E> <S> The sentence that is a song to a music. <E> <S> My wife is a big thought, but I don't support thats all any pool. <E> <S> I would have stopped from the bad thing about my paranoid the original party togay and I was such to stop a leg <E> <S> I dont have to suck my wife, then it was a straight time. <E> <S> My family seems to buy an animous because it went off. <E> <S> What dave tastel interest of a bird to show it. <E> <S> As a danger and allowed to stop salad but will the term ones the sex <E> <S> I think I had some shows with the person which is there are people this weekend. <E> <S> I hate when the way to see it was a bad station. <E> <S> I wish I'm going to say, \"What dissolves worst time is?\" <E> <S> I hate the protoconal pornow buying the way I have to call it a sentence we have to take it to the chops. <E> <S> My wife always was attanded by a penis <E> <S> All my gardens should be a leg. <E> <S> I wanted a bottle of ass, I don't want to be finishing me. <E> <S> The sex said and a little bad country to get to the bar that song with strike are thinking than spare if a person watching the catch into my friend in the dog, asked that thought they have to stand on to them. <E> <S> I have sex is to see my face on how beard with a line of this pubit and then there was a chicken alley. <E> <S> There are a book about the shoes when there will be mathemited but instead be asked. <E> <S> If it said \"when I have an althemains in my well short,\" it was so but Im stupid on my business things. <E> <S> I've bother asking me what has a person who was a bar, are my family is that the steel is all my child <E> <S> I'm too shopping for my fat on the mirror is a single calendar after another shoulder. <E> <S> When I walked into a bar, this is so body with my bike <E> <S> There was to stop this porn and a sexicone is that I'm sure if they was a big talk back! <E> <S> I think about a sound in an arm and the books start some shirt. <E> <S> I heard an end of since I was as a strung on my friend to go into a big time; and we was worried I'm now\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 10000, top_k=5, prime=\"<S> \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2799cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
